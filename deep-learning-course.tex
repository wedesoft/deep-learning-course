\documentclass{article}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{cleveref}
\usepackage{booktabs}
\usepackage{txfonts}
\usepackage{minted}
\usepackage{mathtools}

\begin{document}
\section{Neural Networks and Deep Learning}
See \url{https://www.deeplearning.ai/} for the course.
\subsection{Introduction}
Machine learning has different applications
\begin{itemize}
\item real estate, online advertising (standard neural networks)
\item image classification (CNN)
\item audio to text, language translation (RNN)
\item position of other cars in autonomous driving (hybrid)
\end{itemize}

Supervised learning uses data consisting of input (x) and desired output (y).
In supervised learning there is structured data (house price and features)
and unstructured data (images, audio, text).

Traditional approaches hit a performance plateau as more and more data becomes available.
Deep learning is facilitated by
\begin{itemize}
\item larger datasets
\item more computational power
\item algorithmic improvements
\end{itemize}

Geoffrey Hinton's advice:
\begin{itemize}
\item read enough so that you start developing intuitions
\item trust your intuitions
\item never stop programming
\end{itemize}

\subsection{Basics of Neural Network programming}
\subsubsection{Binary Classification}
Logistic regression is an algorithm for binary classification (e.g. y=1 (cat) vs y=0 (no cat)).
The training set consists of samples $(x,y)$ with $x\in\mathbb{R}^{n_x}$ and $y\in\{0,1\}$.
The training set is $(x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}), \ldots, (x^{(m)},y^{(m)})$ with $m=m_{train}$ (or $m=m_{test}$ for the test set).
The training data is represented by the matrix
\begin{equation}
  X=\begin{pmatrix}x^{(1)} & x^{(2)} & \cdots & x^{(m)}\end{pmatrix}\in\mathbb{R}^{n_x\times m}
\end{equation}
The labels are
\begin{equation}
  Y=\begin{pmatrix}y^{(1)} & y^{(2)} & \cdots & y^{(m)}\end{pmatrix}\in\mathbb{R}^{1\times m}
\end{equation}

Logistic regression tries to model the probability $\hat{y}=P(y=1|x)$.
The model has the parameters $w\in\mathbb{R}^{n_x}$ and $b\in\mathbb{R}$.
The model is $\hat{y}^{(i)}=\sigma(w^\top x^{(i)}+b)$ using $\sigma(z^{(i)})=\frac{1}{1+e^{-z^{(i)}}}$.
Also see \cref{fig:sigmoid}.
\begin{figure}[htbp]
  \begin{center}
    \includegraphics[width=.5\textwidth]{sigmoid}
    \caption{The sigmoid activation function}
    \label{fig:sigmoid}
  \end{center}
\end{figure}

\subsubsection{Cost function}
The logistic regression \emph{loss function} computes the error for a single training example.
\begin{equation}
  \mathcal{L}(\hat{y},y)=-\big(y\log\hat{y}+(1-y)\log(1-\hat{y})\big)
\end{equation}
with $y\in\{0,1\}$.
The \emph{cost function} is the average of the loss function of the entire training set.
\begin{equation}
  J(w,b)=\frac{1}{m}\sum_{i=1}^m\mathcal{L}(\hat{y}^{(i)},y^{(i)})=
  -\frac{1}{m}\sum_{i=1}^m\big[y^{(i)}\log\hat{y}^{(i)}+(1-y^{(i)})\log(1-\hat{y}^{(i)})\big]
\end{equation}

\subsubsection{Gradient descent}
Gradient descent iteratively minimizes $J(w,b)$ using partial derivatives of $J$.
\begin{equation}
  \begin{split}
    w&\coloneqq w-\alpha\underbrace{\frac{\partial J(w,b)}{\partial w}}_{\eqqcolon dw}\\
    b&\coloneqq b-\alpha\underbrace{\frac{\partial J(w,b)}{\partial b}}_{\eqqcolon db}
  \end{split}
\end{equation}
$\alpha$ is the learning rate.

Derivative chain-rule: $\frac{df(g(x))}{dx}=\frac{df}{dg}\frac{dg}{dx}$.
In the source code $\frac{dJ}{dv}$ is named ``dv'' where $J$ is the final output variable.

In the case of logistic regression with two features $z=w_1x_1+w_2x_2+b$ and $a=\sigma(z)$.
\begin{equation}
  \begin{split}
    \frac{d\mathcal{L}(a,y)}{da}&=-\frac{y}{a}+\frac{1-y}{1-a}\\
    \frac{da}{dz}&=a(1-a)\\
    ``dz''=\frac{d\mathcal{L}}{dz}&=\frac{d\mathcal{L}}{da}\frac{da}{dz}=a-y
  \end{split}
\end{equation}
$\frac{d\mathcal{L}}{dw_1}=``dw_1''=x_1 dz$, $``dw_2''=x_2dz$, and $``db''=dz$.
The update rule for gradient descent then is
\begin{equation}
  \begin{split}
    w_1&\coloneqq w_1-\alpha dw_1\\
    w_2&\coloneqq w_2-\alpha dw_2\\
    b&\coloneqq b-\alpha db\\
  \end{split}
\end{equation}

\subsubsection{Logistic regression on $m$ examples}
Logistic regression over multiple training examples works by simply summing up the derivatives.
\begin{equation}
  \frac{\partial}{\partial w_1}J(w,b)=\frac{1}{m}\sum_{i=1}^m\underbrace{\frac{\partial}{\partial w_1}\mathcal{L}(a^{(i)},y^{(i)})}_{dw_1^{(i)}}
\end{equation}
See \cref{fig:logistic-regression} for more detail.
\begin{figure}[htbp]
  \begin{center}
    \includegraphics[width=\textwidth]{logistic-regression}
    \caption{Logistic regression on $m$ examples}
    \label{fig:logistic-regression}
  \end{center}
\end{figure}

\subsubsection{Vectorizing Logistic Regression}
\begin{minted}{python}
# Examples for vectorized code
import numpy as np
u = np.dot(A, v)
u = np.exp(v)
np.log(v)
np.maximum(v, o)
v ** 2
\end{minted}

\begin{equation}
  Z=\begin{bmatrix}z^{(1)} & z^{(2)} & \cdots & z^{(m)}\end{bmatrix}=
    w^\top X + \underbrace{\begin{bmatrix}b & b & \cdots & b\end{bmatrix}}_{\in\mathbb{R}^{1\times m}}
\end{equation}

In Python:
\begin{minted}{python}
  Z = np.dot(w.T, x)+b
\end{minted}

\begin{equation}
  A=\begin{bmatrix}a^{(1)} & a^{(2)} & \cdots & a^{(m)}\end{bmatrix}=\sigma(Z)
\end{equation}

\begin{equation}
  dZ=\begin{bmatrix}dz^{(1)} & dz^{(2)} & \cdots & dz^{(m)}\end{bmatrix}
\end{equation}

\begin{equation}
  dZ=A-Y
\end{equation}

\begin{equation}
  db=\frac{1}{m}\sum_{i=1}^m dz^{(i)}
\end{equation}

\begin{minted}{python}
A = sigmoid(np.dot(w.T, X) + b)
cost = -np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A)) / m
dw = np.dot(X, (A - Y).T) / m
db = np.sum(A - Y) / m
\end{minted}

\begin{equation}
  dw=\frac{1}{m}X dz^\top
\end{equation}

\begin{equation}
  \begin{split}
    w &\coloneqq w-\alpha dw\\
    b &\coloneqq b-\alpha db
  \end{split}
\end{equation}

\subsubsection{Logistic Regression Cost Function}
The two cases
\begin{itemize}
  \item if $y=1$: $p(y|x)=\hat{y}$
  \item if $y=0$: $p(y|x)=1-\hat{y}$
\end{itemize}
can be expressed as one equation: $p(y|x)=\hat{y}^y(1-\hat{y})^{(1-y)}$.
Then
\begin{equation}
  \log p(y|x)=y\log\hat{y}+(1-y)\log(1-\hat{y})
\end{equation}

Overall log-probability to maximise (maximum likelihood)
\begin{equation}
  \log\prod_{i=1}^m p(y^{(i)}|x^{(i)})=\sum_{i=1}^m \underbrace{\log p(y^{(i)}|x^{(i)})}_{-\mathcal{L}(y^{(i)}|x^{(i)})}
\end{equation}
I.e. minimize the negative value
\begin{equation}
  J(w,b)=\frac{1}{m}\sum_{i=1}^m \mathcal{L}(y^{(i)}|x^{(i)})
\end{equation}
which is the cost function.

Interview with Pieter Abbeel on reinforcement learning applied to robotics.

\subsection{One hidden layer Neural Network}
\subsubsection{Neural Network Overview}
\begin{equation}
  \begin{split}
    z&=w^\top x+b\\
    a&=\sigma(z)\\
    \mathcal{L}(a,y)&=-y\log(a)-(1-y)\log(1-a)
  \end{split}
\end{equation}

In a neural network with one hidden layer we have:
\begin{equation}
  \begin{split}
    z^{[1]}&=W^{[1]}x+b^{[1]}\\
    a^{[1]}&=\sigma(z^{[1]})\\
    z^{[2]}&=W^{[2]}a^{[1]}+b^{[2]}\\
    a^{[2]}&=\sigma(z^{[2]})\\
    \mathcal{L}(a^{[2]},y)&=-y\log(a^{[2]})-(1-y)\log(1-a^{[2]})
  \end{split}
\end{equation}

\subsubsection{Neural Network Representation}
There is
\begin{itemize}
  \item an input layer $a^{[0]}=x$ ($x_1, x_2, \ldots)$
  \item zero, one, or more hidden layer(s) $a^{[l]}$ with $l\in\{1,2,\ldots,L-1\}$
  \item an output layer $\hat{y}=a^{[L]}$
\end{itemize}
Only the input layer is not counted, \emph{e.g.} a 2 layer neural network has 1 hidden layer.
\begin{equation}
  W^{[l]}=
  \begin{pmatrix}
  w^{[l]\top}_1\\
  w^{[l]\top}_2\\
  \vdots
  \end{pmatrix}
\end{equation}

\subsubsection{Vectorizing across multiple examples}
\begin{equation}
  \begin{split}
    x^{(1)}&\to a^{[L](1)}=\hat{y}^{(1)}\\
    x^{(2)}&\to a^{[L](2)}=\hat{y}^{(2)}\\
    &\vdots\\
    x^{(m)}&\to a^{[L](m)}=\hat{y}^{(m)}
  \end{split}
\end{equation}
The computation of multiple training examples is vectorized using
\begin{equation}
  X=\begin{pmatrix}x^{(1)} & x^{(2)} & \cdots & x^{(m)}\end{pmatrix}\in\mathbb{R}^{n_x\times m}
\end{equation}
(by stacking vectors horizontally) as follows
\begin{equation}
  \begin{split}
    Z^{[1]}&=W^{[1]}X+b^{[1]}\\
    A^{[1]}&=\sigma(Z^{[1]})\\
    Z^{[2]}&=W^{[2]}A^{[1]}+b^{[2]}\\
    A^{[2]}&=\sigma(Z^{[2]})
  \end{split}
\end{equation}

\subsubsection{Activation Functions}
\begin{itemize}
  \item $\sigma(z)=\frac{1}{1+e^{-z}}\in[0,1]$
  \item $\tanh z=\frac{e^z-e^{-z}}{e^z+e^{-z}}\in[-1,1]$
  \item $\operatorname{ReLU}(z)=\max(0, z)$
\end{itemize}

The $\tanh$ function generally is better than $\sigma$ because the mean of the output is zero.
One exception is the output layer where inputs between zero and one are desirable.

\subsubsection{Derivatives of Activation Functions}
\begin{equation}
  g(z)=\frac{1}{1+e^{-z}}\to g^\prime(z)=g(z)(1-g(z))
\end{equation}
\begin{equation}
  g(z)=\tanh{z}\to g^\prime(z)=1-\big(g(z)\big)^2
\end{equation}
\begin{equation}
  g(z)=\max(0, z)\to g^\prime(z)=\left\{\begin{array}{ll}0&\mathrm{\ if\ }z<0\\1&\mathrm{\ if\ }z>0\end{array}\right.
\end{equation}

\subsubsection{Gradient descent for neural networks}\label{cha:gradneural}
Forward propagation:
\begin{equation}
  \begin{split}
    Z^{[1]}&=W^{[1]}X+b^{[1]}\\
    A^{[1]}&=g^{[1]}(Z^{[1]})\\
    Z^{[2]}&=W^{[2]}A^{[1]}+b^{[2]}\\
    A^{[2]}&=g^{[2]}(Z^{[2]})=\sigma(Z^{[2]})
  \end{split}
\end{equation}
Backpropagation:
\begin{equation}
  \begin{split}
    dZ^{[2]}&=A^{[2]}-Y\\
    dW^{[2]}&=\frac{1}{m}dZ^{[2]}A^{[1]\top}\\
    db^{[2]}&=\frac{1}{m}\sum_i dZ^{[2](i)}\\
    dZ^{[1]}&=W^{[2]\top}dZ^{[2]}*g^{[1]\prime}(Z^{[1]})\\
    dW^{[1]}&=\frac{1}{m}dZ^{[1]}X\\
    db^{[1]}&=\frac{1}{m}\sum_i dZ^{[1](i)}
  \end{split}
\end{equation}
where ``*'' denotes the elementwise product.

\subsubsection{Random Initialization}
Initializing the weights of a neural network to zero is not sufficient.
The activations will be the same.
By initializing the weights randomly, one can ensure that they are linear independent.
\emph{I.e.} it is necessary to break the symmetry.
Usually the weights are initialised to small random values using a Gaussian random distribution with \emph{e.g.} $\sigma=0.01$.
The bias units can be initialised to zero.

Interview with Ian Goodfellow (Generative Adversarial Neural Networks).
His advice: apply the knowledge to something you are interested in, while learning from the book/online lectures.

\subsection{Deep Neural Networks}
Logistic regression is a ``shallow'' neural network.
A neural network with 5 layers can be called a ``deep'' neural network.
\begin{itemize}
  \item $L$ is the number of layers.
  \item $n^{[l]}$ is the number of units in layer $l$.
  \item $n^{[0]}=n_x$ is the number of input units.
  \item $a^{[l]}=g^{[l]}(z^{[l]})$ is the activation at layer $l$
  \item $a^{[0]}=x$ is the input
  \item $a^{[L]}=\hat{y}$ is the output
  \item $z^{[l]}=W^{[l]}a^{[l-1]}+b^{[l]}$
\end{itemize}

\subsubsection{Forward propagation in a deep network}
\begin{equation}
  \begin{split}
    z^{[1]}&=W^{[1]}x+b^{[1]}\\
    a^{[1]}&=g^{[1]}(z^{[1]})\\
    z^{[2]}&=W^{[2]}a^{[1]}+b^{[2]}\\
    a^{[2]}&=g^{[2]}(z^{[2]})\\
    &\vdots\\
    z^{[L]}&=W^{[L]}a^{[L-1]}+b^{[L]}\\
    \hat{y}&=g^{[L]}(z^{[L]})
  \end{split}
\end{equation}
Using $a^{[0]}=x$ and $a^{[L]}=\hat{y}$ one can write in general:
\begin{equation}
  \begin{split}
    z^{[l]}&=W^{[l]}a^{[l-1]}+b^{[l]}\\
    a^{[l]}&=g^{[l]}(z^{[l]})
  \end{split}
\end{equation}
The vectorized form for a training set (by stacking training examples horizontally):
\begin{equation}
  \begin{split}
    Z^{[l]}&=W^{[l]}A^{[l-1]}+b^{[l]}\\
    A^{[l]}&=g^{[l]}(Z^{[l]})
  \end{split}
\end{equation}

\subsubsection{Getting your matrix dimensions right}
The matrix dimensions are as follows:
\begin{itemize}
  \item $W^{[l]}\in\mathbb{R}^{n^{[l]}\times n^{[l-1]}}$
  \item $b^{[l]}\in\mathbb{R}^{n^{[l]}\times 1}$
  \item $z^{[l]},a^{[l]},dz^{[l]},da^{[l]}\in\mathbb{R}^{n^{[l]}\times 1}$
  \item $Z^{[l]},A^{[l]},dZ^{[l]},dA^{[l]}\in\mathbb{R}^{n^{[l]}\times m}$
\end{itemize}

\subsubsection{Why deep representations}
Using the example of learning $x_1\operatorname{xor}x_2\operatorname{xor}\ldots x_n$ one can show that
it can be represented using a deep neural network with $O(\log n)$ layers and $O(n)$ units.
When using only one hidden layer, $O(2^n)$ units are required.

\subsubsection{Building blocks of of deep neural networks}
See \cref{fig:deepblocks}.
\begin{figure}[htbp]
  \begin{center}
    \includegraphics[width=\textwidth]{deepblocks}
    \caption{Building blocks of deep neural network}
    \label{fig:deepblocks}
  \end{center}
\end{figure}

\subsubsection{Forward and Backward Propagation}
\textbf{Forward propagation} for layer $l$:
\begin{itemize}
  \item Input: $a^{[l-1]}$
  \item Output: $a^{[l]}$, cache: $z^{[l]}$
\end{itemize}
The implementation is
\begin{equation}
  \begin{split}
    z^{[l]}&=W^{[l]}a^{[l-1]}+b^{[l]}\\
    a^{[l]}&=g^{[l]}(z^{[l]})
  \end{split}
\end{equation}
The vectorized version simply is
\begin{equation}
  \begin{split}
    Z^{[l]}&=W^{[l]}A^{[l-1]}+b^{[l]}\\
    A^{[l]}&=g^{[l]}(Z^{[l]})
  \end{split}
\end{equation}
where $A^{[0]}=X$.

\textbf{Backward propagation} for layer $l$:
\begin{itemize}
  \item Input: $da^{[l]}$, cached: $z^{[l]}$
  \item Output: $da^{[l-1]}$, $dW^{[l]}$, $db^{[l]}$
\end{itemize}
The implementation is
\begin{equation}
  \begin{split}
    dz^{[l]}&=da^{[l]}*g^{[l]\prime}(z^{[l]})\\
    dW^{[l]}&=dz^{[l]}a^{[l-1]}\\
    db^{[l]}&=dz^{[l]}\\
    da^{[l-1]}&=W^{[l]\top}dz^{[l]}
  \end{split}
\end{equation}
where $da^{[L]}=-\frac{y}{a}+\frac{1-y}{1-a}$.
Note that $dz^{[l]}=W^{[l+1]\top}dz^{[l+1]}*g^{[l]\prime}(z^{[l]})$ similar as in \cref{cha:gradneural}.
The vectorized version is
\begin{equation}
  \begin{split}
    dZ^{[l]}&=dA^{[l]}*g^{[l]\prime}(Z^{[l]})\\
    dW^{[l]}&=\frac{1}{m}dZ^{[l]}A^{[l-1]\top}\\
    db^{[l]}&=\frac{1}{m}\sum_i dZ^{[l](i)}=\frac{1}{m}np.sum(dZ^{[l]}, axis=1,keepdims=True)\\
    dA^{[l-1]}&=W^{[l]\top}dZ^{[l]}
  \end{split}
\end{equation}
where $dA^{[L]}=\sum_{i=1}^m-\frac{y^{(i)}}{a^{(i)}}+\frac{1-y^{(i)}}{1-a^{(i)}}$.

\subsubsection{Parameters vs. Hyperparameters}
$W^{[l]}$ and $b^{[l]}$ with $l\in\{1,2,\ldots,L\}$ are parameters.
Hyperparameters are:
\begin{itemize}
  \item learning rate $\alpha$
  \item number of iterations
  \item number of layers $L$
  \item number of hidden units in each layer
  \item choice of activation function
  \item momentum
  \item minibatch size
  \item regularization parameters
\end{itemize}
Hyperparameters determine the value of the parameters.

\section{Improving Deep Neural Networks: Hyperparameter tuning, Regularisation}
\subsection{Setting up your machine learning application}
\subsubsection{Train / Dev / Test sets}
Split up your data randomly into
\begin{itemize}
  \item training set
  \item hold-out cross-validation set (development set)
  \item test set
\end{itemize}
Traditionally the split was 60\%/20\%/20\% for data sets with 100, 1000, or 10000 samples.
In the modern big data era (1000000 samples) the split can be 98\%/1\%/1\%.

A common problem is a mismatched train/test distribution.
A rule of thumb is to ensure that dev and test set are from the same distribution.

Not having a test set might be okay (if no unbiased estimate is needed).

\subsubsection{Bias/Variance}
The training and dev set error are the main indicator to distinguish between
bias (underfitting) and variance (overfitting) (see \cref{tbl:biasvariance})
\begin{table}[htbp]
  \begin{center}
    \begin{tabular}{crrrr}\toprule
      \textbf{train set performance} &  1\% & 15\% & 15\% & 0.5\%\\
      \textbf{dev set performance}   & 11\% & 16\% & 30\% & 1\%\\\midrule
      & high variance & high bias & both high & both low\\\bottomrule
    \end{tabular}
    \caption{Examples of bias and variance\label{tbl:biasvariance}}
  \end{center}
\end{table}
Also see \cref{fig:regularisation}.
\begin{figure}[htbp]
  \begin{center}
    \includegraphics[width=\textwidth]{regularisation}
    \caption{Underfitting and overfitting}
    \label{fig:regularisation}
  \end{center}
\end{figure}

\subsubsection{Basic Recipe for Machine Learning}
When encountering high \emph{bias}:
\begin{itemize}
  \item bigger network
  \item train longer
  \item neural network architecture search
\end{itemize}
When encountering high \emph{variance}:
\begin{itemize}
  \item more data
  \item regularization
  \item neural network architecture search
\end{itemize}

\subsection{Regularizing your neural network}
\subsubsection{Regularization}
When using logistic regression with $L_2$ regularization one minimizes
\begin{equation}
  J(w,b)=\frac{1}{m}\sum_{i=1}^m\mathcal{L}(\hat{y}^{(i)},y^{(i)})+\frac{\lambda}{2m}||w||_2^2
\end{equation}
where $w\in\mathbb{R}^{n_x}$, $b\in\mathbb{R}$, and $||w||_2^2=\sum_{j=1}^{n_x}w_j^2=w^\top w$
(\emph{i.e.} using the $L_2$ norm).
$\lambda$ is the regularization parameter.

When using a neural network, regularization is implemented as follows
\begin{equation}
  J(W^{[1]},b^{[1]},\ldots,W^{[L]},b^{[L]})=\frac{1}{m}\sum_{i=1}^m\mathcal{L}(\hat{y}^{(i)},y^{(i)})
  +\frac{\lambda}{2m}\sum_{l=1}^L||W^{[l]}||_F^2
\end{equation}
where $||W^{[l]}||_F^2=\sum_{i=1}^{n^{[l]}}\sum_{j=1}^{n^{[l-1]}}(W^{[l]}_{ij})^2$ (Frobenius norm) and
$W^{[l]}\in\mathbb{R}^{n^{[l]}\times n^{[l-1]}}$.
It follows that $dW^{[l]}=(\mathrm{from\ backprop})+\frac{\lambda}{m}W^{[l]}$.
$L_2$ regularization sometimes is also called ``weight decay''.

\subsubsection{Dropout regularization}
Dropout randomly eliminates units from the network during training.
Dropout can be implemented using ``inverted dropout''.
\emph{E.g.} for layer 3 using keep\_prob=0.8.
\begin{minted}{python}
d3 = np.random.rand(a3.shape[0], a3.shape[1]) < keep_prob
\end{minted}
The activations a3 are multiplied with the resulting boolean array.
\begin{minted}{python}
a3 = np.multiply(a3, d3)  # a3 *= d3
a3 /= keep_prob
\end{minted}
Note that a3 is scaled so that the expected value for $Z^{[4]}$ does not change.
When doing backpropagation, dA is multiplied with the same boolean array and scaled with the same factor.
\begin{minted}{python}
da3 = np.multiply(da3, d3)
da3 /= keep_prob
\end{minted}
At test time dropout is \emph{not used}.
keep\_prob can have different values for each layer.

\subsubsection{Why does drop-out work?}
Intuition: Each unit can't rely on any one feature, so have to spread out weights.
The downside of dropout is that the cost function $J$ is not well defined any more.

\subsubsection{Other regularization methods}
\textbf{Data augmentation:}
\begin{itemize}
  \item given images one can flip them horizontally to double the data available
  \item one can crop images or randomly distort or rotate them to synthesize more images
\end{itemize}
\textbf{Early stopping:}
The training and dev set error are plotted.
Usually the dev set error while go down for a while and then eventually increases from there.
One can stop before the dev set error goes up.
The downside of early stopping is that the tasks of minimizing $J$ and not overfitting are coupled
and cannot be worked on independently any more.

\subsection{Setting up your optimization problem}
\subsubsection{Normalizing Inputs}\label{cha:norminputs}
Normalizing the inputs consists of two steps:
\begin{enumerate}
  \item subtract the mean $\mu = \frac{1}{m}\sum_{i=1}^m x^{(i)}$
  \item normalize the variance $\sigma^2=\frac{1}{m}\sum_{i=1}^m(x^{(i)}-\mu)^2$
\end{enumerate}
Note that you should use the same $\mu$ and $\sigma$ for the test set as you use for the training set.
Normalizing the input features improves the symmetry of the cost function and allows you to use a faster learning rate.

\subsubsection{Vanishing/Exploding Gradients}
The slopes in deep networks can get very large or very small.

\subsubsection{Weight Initialization for Deep Networks}
The weights for a single neuron $z=w_1x_1+w_2x_2+\ldots+w_nx_n$ should be initialised with $Var(w_i)=\frac{2}{n}$.
\begin{equation}
  W^{[l]}=np.random.randn(shape)*np.sqrt(\frac{2}{n^{[l-1]}})
\end{equation}
where $n^{[l-1]}$ is the number of inputs for layer $l$.
The optimal variance depends on the activation function.
\begin{itemize}
  \item for $\tanh$ use variance of $\sqrt{\frac{1}{n^{[l-1]}}}$
  \item for ReLU use variance of $\sqrt{\frac{2}{n^{[l-1]}}}$
\end{itemize}
The variance for random initialisation can also be a hyperparameter to optimize.

\subsubsection{Numerical approximation of gradients}
$f^\prime(\theta)$ can be estimated using
$f^\prime(\theta)\approx\frac{f(\theta+\epsilon)-f(\theta-\epsilon)}{2\epsilon}$ where $\epsilon$ is a small value.

\subsubsection{Gradient checking}
\begin{itemize}
  \item Take $W^{[1]},b^{[1]},\ldots,W^{[L]},b^{[L]}$ and reshape into a big vector $\theta$.
  \item Take $dW^{[1]},db^{[1]},\ldots,dW^{[L]},db^{[L]}$ and reshape into a big vector $d\theta$.
  \item For each $i$:
    \begin{itemize}
      \item $d\theta_{approx}[i]=
        \frac{J(\theta_1,\theta_2,\ldots,\theta_i+\epsilon,\ldots)- J(\theta_1,\theta_2,\ldots,\theta_i-\epsilon,\ldots)}{2\epsilon}
        \approx d\theta[i]=\frac{\partial J}{\partial\theta_i}$
    \end{itemize}
  \item Check $\frac{||d\theta_{approx}-d\theta||_2}{||d\theta_{approx}||_2+||d\theta||_2}$ is similar order of magnitude as  $\epsilon$
\end{itemize}
If the gradient check fails, one can investigate which component of $d\theta$ is substantially different to
the corresponding component of $d\theta_{approx}$.
Remember to include the regularization term if regularization is used.
Note that gradient checking does not work with dropout.
One can run gradient checking after initialization and then again after having run training for some time.

Yoshua Bengio: Reinforcement learning, how does a human explore and understand the world?
How does causality and high-level abstractions work?
Implement things yourself. Read ICLR proceedings.

\subsection{Optimization Algorithms}
\subsubsection{Mini-batch gradient descent}
Mini-batch gradient descent is a faster gradient descent algorithm.
Instead of using all $m$ examples, the training set is split up into mini-batches
$(X^{\{1\}},Y^{\{2\}}), (X^{\{2\}},Y^{\{2\}}), \ldots$ which get processed sequentially.
A single pass through the training set is called an ``epoch''.

\subsubsection{Understanding mini-batch gradient descent}
Unlike batch gradient descent, when using mini-batch gradient descent, the cost function $J$ does not decrease monotonically.
\emph{Stochastic gradient descent} uses a mini-batch size of $1$.
In practise one uses a mini-batch size between the to extremes $1$ and $m$.
The reason is that vectorization is still used and the convergence is faster.
\begin{itemize}
  \item if your training set is small, just use batch gradient descent ($m\le 2000$)
  \item typical mini-batch sizes: $64,128,\ldots,1024$
  \item make sure $(X^{\{t\}},Y^{\{t\}})$ fits into CPU/GPU memory
\end{itemize}
Mini-batch size can be seen as a hyper parameter to optimize.

\subsubsection{Exponentially weighted averages}
One can compute an exponentially weighted average of a sequence $\theta_1,\theta_2,\ldots$
using $v_0=0$ and $v_t=\beta\theta_{t-1}+(1-\beta)\theta_t$.

\subsubsection{Bias correction in exponentially weighted averages}
$v_t=\beta v_{t-1}+(1-\beta)\theta_t$ starts at $0$, \emph{i.e.} is biased initially.
The bias corrected value is $\frac{v_t}{1-\beta^t}$.

\subsubsection{Gradient descent with momentum}
Exponentially weighted averages of the gradient can be used to update the parameters.
This allows you to increase the learning rate.
\begin{equation}
  \begin{split}
    V_{dW}=&\beta V_{dW}+(1-\beta)dW\\
    V_{db}=&\beta V_{db}+(1-\beta)db\\
    W&\coloneqq W-\alpha V_{dW}\\
    b&\coloneqq b-\alpha V_{db}
  \end{split}
\end{equation}
Gradient descent with momentum has the two hyperparameters $\alpha$ and $\beta$.
Typical value for $\beta$ is $0.9$.

\subsubsection{RMSprop}
On iteration $t$:
\begin{itemize}
  \item Compute $dW$, $db$ on current mini-batch
  \item $s_{dW}=\beta_2 s_{dW}+(1-\beta_2)dW^2$ (exponential average of element-wise square)
  \item $s_{db}=\beta_2 s_{db}+(1-\beta_2)db^2$ (exponential average of element-wise square)
  \item $W\coloneqq W-\alpha\frac{dW}{\sqrt{s_{dW}}+\epsilon}$
  \item $b\coloneqq b-\alpha\frac{db}{\sqrt{s_{db}}+\epsilon}$
\end{itemize}
$\epsilon$ is used to prevent division by zero.

\subsubsection{Adam optimization algorithm}
The Adam\footnote{Adaptive moment estimation} optimization algorithm is one of the few optimization algorithms
which work well on a wide range of problems.
The Adam optimization algorithm basically combines gradient descent with momentum and RMSprop.
\begin{itemize}
  \item $V_{dW}=0$, $s_{dW}=0$, $V_{db}=0$, $s_{db}=0$
  \item On iteration $t$:
    \begin{itemize}
      \item compute $dW$, $db$ using current mini-batch
      \item $V_{dW}=\beta_1 V_{dW}+(1-\beta_1)dW$, $V_{db}=\beta_1 V_{db}+(1-\beta_1)db$
      \item $s_{dW}=\beta_2 s_{dW}+(1-\beta_2)dW^2$, $s_{db}=\beta_2 s_{db}+(1-\beta_2)db^2$
      \item $V_{dW}^{corrected}=V_{dW}/(1-\beta_1^t)$, $V_{db}^{corrected}=V_{db}/(1-\beta_1^t)$
      \item $s_{dW}^{corrected}=s_{dW}/(1-\beta_2^t)$, $s_{db}^{corrected}=s_{db}/(1-\beta_2^t)$
      \item $W\coloneqq W-\alpha\frac{V_{dW}^{corrected}}{\sqrt{s_{dW}^{corrected}}+\epsilon}$,
        $b\coloneqq b-\alpha\frac{V_{db}^{corrected}}{\sqrt{s_{db}^{corrected}}+\epsilon}$
    \end{itemize}
\end{itemize}
The hyperparameter $\alpha$ needs to be tuned.
A common choice for $\beta_1$ is $0.9$ (first moment) and for $\beta_2$ is $0.999$ (second moment).
One can use $\epsilon=10^{-8}$.

\subsubsection{Learning rate decay}
One can slowly reduce the learning rate over time to reduce the noise introduced by mini-batches
when approaching the minimum.
\begin{equation}
  \alpha=\frac{1}{1+\mathrm{decay\ rate}*\mathrm{epoch\ num}}\alpha_0
\end{equation}
Other learning rate decay methods are
\begin{itemize}
  \item $\alpha=0.95^{\mathrm{epoch\ num}}\alpha_0$ (exponential decay)
  \item $\alpha=\frac{k}{\sqrt{\mathrm{epoch\ num}}}\alpha_0$ or $\frac{k}{\sqrt{t}}\alpha_0$
\end{itemize}

\subsubsection{The problem of local optima}
High-dimensional spaces rather have saddle points than local minima.
However plateaus can slow down learning.

\subsection{Hyperparameter tuning}
\subsubsection{Tuning process}
\begin{itemize}
  \item $\alpha$
  \item $\beta$
  \item $\beta_1$, $beta_2$, $\epsilon$
  \item \#layers
  \item \#hidden units
  \item learning rate decay
  \item mini-batch size
\end{itemize}
When trying out combinations of hyperparameters, use random values instead of sampling a grid.
This way one tries out more distinct values of each hyperparameter.
Additionally one can use a coarse to fine scheme. \emph{I.e.} sampling more densely where the best results were found.

\subsubsection{Using an appropriate scale to pick hyperparameters}
For number of hidden units and number of layers one can sample a uniform random distribution.
In the case of the learning rate $\alpha$ one uses a logarithmic scale.
\emph{E.g.} $\alpha=10^r$ where $r$ is sampled from a uniform random distribution between $-4$ and $0$.
In a similar fashion one can sample $\beta=1-10^r$ where $r$ is sampled from a uniform random distribution between $-3$ and $-1$.

\subsubsection{Hyperparameter tuning in practice: Pandas vs. Caviar}
Re-test hyperparameters occassionally (after acquiring more data for example).
You can either ``babysit'' (update parameters during training) a single model if a lot of processing is required (Panda approach).
If many models can be trained in parallel, one can simply pick the best one (Caviar approach).

\subsection{Batch Normalization}
\subsubsection{Normalizing activations in a network}
Normalizing inputs speeds up learning (see \cref{cha:norminputs}).
In a similar fashion one can normalize the values $Z^{[l]}$ in each layer $l$.
Given the values $z^{(1)},\ldots,z^{(m)}$ of a layer, one computes
\begin{equation}
  \begin{split}
    \mu&=\frac{1}{m}\sum_i z^{(i)}\\
    \sigma^2&=\frac{1}{m}\sum_i (z^{(i)}-\mu)^2\\
    z^{(i)}_{norm}&=\frac{z^{(i)}-\mu}{\sqrt{\sigma^2+\epsilon}}\\
    \tilde{z}^{(i)}&=\gamma z^{(i)}_{norm}+\beta
  \end{split}
\end{equation}
where $\gamma$ and $\beta$ are learnable parameters of the model.
The values $\tilde{z}^{(i)}$ are then used instead of $z^{(i)}$ for continuing forward propagation.

\subsubsection{Fitting Batch Norm into a neural network}
\begin{equation}
  \begin{split}
    z^{[l]}&=W^{[l]}a^{[l-1]}\\
    \tilde{z}^{[l]}&=\gamma^{[l]}\frac{z^{[l]}-\mu}{\sqrt{\sigma^2+\epsilon}}+\beta^{[l]}\\
    a^{[l]}&=g^{[l]}(\tilde{z}^{[l]})
  \end{split}
\end{equation}
The vector values $\mu$ and $\sigma$ of each layer are computed for the current mini-batch.
Note that $b^{[l]}$ is set to zero because it is made redundant by the vector $\beta^{[l]}$.

\subsubsection{Batch norm at test time}
When testing the neural network, values for $\mu$ and $\sigma^2$ are required.
The values for $\mu$ and $\sigma^2$ can be obtained by computing an exponentially weighted average
of those values at training time.

\subsection{Multi-class classification}
\subsubsection{Softmax regression}
Softmax regression is a generalisation of logistic regression to multiple classes ($C$ classes).
\emph{E.g.} recognising four classes cats, dogs, baby chicks, and other.
In this case the output layer has $C=4$ units ($n^{[L]}=4=C$).
\emph{I.e.} $\hat{y}\in\mathbb{R}^C$.
The softmax activation function are as follows:
\begin{equation}
  \begin{split}
    Z^{[L]}&=W^{[L]}a^{[L-1]}+b^{[L]}\\
    t&=e^{(Z^{[L]})}\\
    a_i^{[L]}&=\frac{t_i}{\sum_{j=1}^C t_j}
  \end{split}
\end{equation}

\subsubsection{Training a softmax classifier}
Note that softmax regression with $C=2$ reduces to logistic regression.
The loss function for softmax regression is
\begin{equation}
  \mathcal{L}(\hat{y},y)=-\sum_{j=1}^C y_j\log\hat{y}_j
\end{equation}
where $y$ always is a basis vector with a single one and the rest zeros.
The cost function is
\begin{equation}
  J(W^{[1]},b^{[1]},\ldots)=\frac{1}{m}\sum_{i=1}^m\mathcal{L}(\hat{y}^{(i)},y^{(i)})
\end{equation}
In a vectorized implementation $Y\in\mathbb{R}^{C\times m}$.

The backpropagation step is (similar as for logistic regression in \cref{cha:gradneural})
\begin{equation}
  \frac{\partial J}{\partial z^{[L]}}=\hat{y}-y
\end{equation}

\subsection{Introduction to programming frameworks}
\subsubsection{Deep learning frameworks}
\begin{itemize}
  \item Caffe/Caffe2
  \item CNTK
  \item DL4J
  \item Keras
  \item Lasagne
  \item mxnet
  \item PaddlePaddle
  \item Tensorflow
  \item Theano
  \item Torch
\end{itemize}
Important criteria when choosing a deep learning framework are
\begin{itemize}
  \item ease of programming (development and deployment)
  \item running speed
  \item truly open (open source with good governance)
\end{itemize}

\subsubsection{Tensorflow}
The example is to minimize
\begin{equation}
  J(w)=w^2-10w+25
\end{equation}
\begin{minted}{python}
import tensorflow as tf
w = tf.Variable(0, dtype=tf.float32)
cost = tf.add(tf.add(w ** 2, tf.multiply(-10, w)), 25)
train = tf.train.GradientDescentOptimizer(0.01).minimize(cost)
init = tf.global_variables_initializer()
session = tf.Session()
session.run(init)
for i in range(1000):
  session.run(train)
print(session.run(w))
\end{minted}
It is only necessary to define the forward propagation.
Tensorflow is able to derive the derivatives necessary for the backward propagation step.
For training data one uses a placeholder. For example
\begin{minted}{python}
x = tf.placeholder(tf.float32, [3, 1])
coefficients = np.array([[1.], [-10.], [25.]])
session.run(train, feed_dict={x: coefficients})
\end{minted}

\section{Structuring Machine Learning Projects}
\subsection{Introduction to ML Strategy}
\subsubsection{Why ML Strategy}
It is important to have a quick and effective way to figure out which of all the ideas are worth pursuing and which you can discard.

\subsubsection{Orthogonalization}
It is important that the different controls of the machine learning system have interpretable effects.
\emph{I.e.} one wants to change one thing at a time.
Things to control in machine learning are
\begin{itemize}
  \item fit training set well on cost function (train bigger network, use better optimization algorithm)
  \item fit dev set well on cost function (regularization, bigger training set)
  \item fit test set well on cost function (bigger dev set)
  \item performs well in real world (change dev set or the cost function)
\end{itemize}
In contrast early stopping simultaneously affects training and dev set performance.

\subsection{Setting up your goal}
\subsubsection{Single number evaluation metric}
It is recommended to setup a single number evaluation metric to measure the performance of your algorithm.
\begin{itemize}
  \item precision: what percentage of the classified examples are correctly classified
  \item recall: what percentage of the available class examples are detected
\end{itemize}
A single number evaluation metric represents the trade-off between precision and recall
(i.e. use F1 score $\frac{2}{\frac{1}{P}+\frac{1}{R}}$ ``harmonic mean'').

\subsubsection{Satisficing and Optimizing metric}
Example: maximize accuracy subject to running time $\le 100ms$.
Then the accuracy is an optimizing metric while the running time is a satisficing metric.
In general you can have one optimizing metric and many satisficing metrics.
Another example is to optimize the accuracy subject to a certain limit on the number of false positives.

\subsubsection{Train/dev/test distributions}
It is important for the training and dev set to come from the same distribution.
The dev set together with the evaluation metric are the target for development of the algorithm.
\textbf{Choose a dev set and test set to reflect data you expect to get in the future and consider important to do well on.}

\subsubsection{Size of the dev and test sets}
The old way of splitting data 60\%/20\%/20\% applies to small test sets ($\le 10000$ samples).
When there is sufficient data (\emph{e.g.} 1000000 samples) then one can split 98\%/1\%/1\%.
Set your test set to be big enough to give high confidence in the overall performance of your system

\subsubsection{When to change dev/test sets and metrics}
Sometimes when trying the algorithm in the real world, one realises that one has to change the evaluation metric.
\emph{E.g.} you can weight some samples in the dev and test set more strongly.
It is important to change the metric to capture your preferences instead of coasting with an old metric.
There are two orthogonal tasks:
\begin{enumerate}
  \item set target, define where you want to aim
  \item do well on this metric
\end{enumerate}
If doing well on your metric + dev/test set does not correspond to doing well on your application, change your metric and/or dev/test set.
An example is to increase weights of pornographic images to ensure they don't get included in the wrong class.

\subsection{Comparing to human-level performance}
\subsubsection{Why human-level performance?}
The theoretical limit for the error (Bayes optimal error) of an algorithm generally is not 100\%
(\emph{e.g.} noisy audio, blurry images).
As long as your algorithm performs worse than a human, you can get more labelled data from humans to train your algorithm.
\begin{itemize}
  \item Get labelled data from humans
  \item Gain insights from manual error analysis: Why did a person get this right?
  \item better analysis of bias/variance
\end{itemize}

\subsubsection{Avoidable bias}
Bayes optimal error can only be overcome by overfitting.
Often human level performance is used as an estimate for Bayes optimal error.
In the following example the decision on what to do is different depending on the human level performance
even though training and dev set performance are the same (see \cref{tbl:human}).
\begin{table}[htbp]
  \begin{center}
    \begin{tabular}{r|r|r}
      human level performance     & 1\%  & 7.5\% \\
      training set performance    & 8\%  & 8\%   \\
      development set performance & 10\% & 10\%  \\\hline
      decision & reduce bias & reduce variance
    \end{tabular}
    \caption{Different decision based on human level performance\label{tbl:human}}
  \end{center}
\end{table}
The difference between training set error and Bayes optimal error can be called ``avoidable bias''.

\subsubsection{Understanding human-level performance}
Human-level performance often is used as a proxy for Bayes error.
In this case the best result (\emph{e.g.} by a group of experts) is used.
In other cases human-level performance is used to show that the system has comparable performance to an expert.
In this case the comparative result (\emph{e.g.} single expert) is enough.
As you approach human-level error it becomes harder to know whether you have a bias or variance problem,
because the Bayes optimal error is not known.

\subsubsection{Surpassing human-level performance}
Algorithms surpass humans at
\begin{itemize}
  \item online advertising
  \item product recommendations
  \item logistics (predicting transit time)
  \item loan approvals
\end{itemize}
All this examples are using structured data (\emph{i.e.} these are not natural perception tasks).
Furthermore in these application a large amount of data is available.

\subsubsection{Improving your model performance}
The two fundamental assumptions of supervised learning
\begin{enumerate}
  \item You can fit the training set pretty well.
  \item The training set performance generalizes pretty well to the dev/test set.
\end{enumerate}
Reducing avoidable bias
\begin{itemize}
  \item train bigger model
  \item train longer/use better optimization algorithm
  \item NN architecture/hyperparameters search (\emph{e.g.} RNN, CNN)
\end{itemize}
Reducing variance
\begin{itemize}
  \item more data
  \item regularization (L2, dropout, data augmentation)
  \item NN architecture/hyperparameters search
\end{itemize}

Andrej Karpathy: understand machine learning algorithms from scratch, CS231n lectures.

\subsection{Error Analysis}
\subsubsection{Carrying out error analysis}
Manually examining examples where the algorithm is not performing well can give you insights on what to do next.
\emph{I.e.} get 100 mislabelled dev set examples and analyse what kind of mistakes are most common.
For example
\begin{itemize}
  \item fix pictures of dogs being recognized as cats
  \item fix great cats (lions, panthers, etc.) being misrecognized
  \item improve performance on blurry images
\end{itemize}
This helps to prioritize what class of error to work on.

\subsubsection{Cleaning up incorrectly labelled data}
Deep learning algorithms are quite robust to random errors in the training set.
In contrast systematic errors (\emph{e.g.} white dogs classified as cats) are problematic.
During error analysis one can treat incorrectly labelled examples simply as an other class of error.
When correcting incorrect dev/test set examples
\begin{itemize}
  \item apply same process to your dev and test sets to make sure they continue to come from the same distribution
  \item consider examining examples your algorithm got right as well as ones it got wrong
  \item train and dev/test set data may now come from slightly different distributions
\end{itemize}

\subsubsection{Build your first system quickly, then iterate}
\begin{itemize}
  \item Set up dev/test set and metric
  \item build initial system quickly
  \item use bias/variance analysis \& error analysis to prioritize next steps
\end{itemize}

\subsection{Mismatched training and dev/test set}
\subsubsection{Training and testing on different distributions}
It is important that the dev/test set are representative of the application you care about!
\emph{E.g.} dev/test set with data from mobile application and training set mostly data from web pages.

\subsubsection{Bias and Variance with mismatched data distributions}
If training set and dev set are from different distributions it is necessary to carve out (after random shuffling)
a piece of the training set (a training-dev set) in order to be able to distinguish variance from data mismatch in the dev set.
As before you can use human level error as a proxy for Bayes optimal error to estimate avoidable bias.

If the dev-set performance is much better than the test-set performance, you are overfitting the dev-set and you need to find a larger dev-set.

There is human-level error on training data, error on training data, and error on training-dev data are three numbers.
The human-level error on dev/test-data, error on part of training data, and error on dev/test data are a further three numbers.
Also see \cref{fig:mismatch}.
\begin{figure}[htbp]
  \begin{center}
    \includegraphics[width=\textwidth]{mismatch}
    \caption{Different values to consider when dealing with two distributions of data}
    \label{fig:mismatch}
  \end{center}
\end{figure}

\subsubsection{Adressing data mismatch}
\begin{itemize}
  \item Carry out error analysis to try to understand the difference between training and dev set
  \item Make training data more similar, or collect more data similar to dev/test set
\end{itemize}
You can use artificial data synthesis to make the training data more similar to the dev set.
Note that synthesized data can lack in variation and might only create a small subset of the data required
which causes the neural network to overfit.
\emph{E.g.} if you have 100,000 hours of speech data and use only 1 hour of car noise to synthesize data, the neural network might overfit.

\subsection{Learning from multiple tasks}
\subsubsection{Transfer learning}
You can replace the last layer of a \emph{pre-trained} network with random weights and then train
(\emph{fine tune}) the network (or the final layer(s) of a network) on a different task.
If the new data set is small, you can restrict training to the part of the neural network.
Transfer learning from task A to task B makes sense when
\begin{itemize}
  \item task A and task B have the same input
  \item you have a lot more data for task A than task B
  \item low level features from A could be helpful for learning B
\end{itemize}

\subsubsection{Multi-task learning}
Multi-task learning is to apply one neural network to multiple tasks simultaneously.
\emph{E.g.}
\begin{itemize}
  \item detect pedestrians
  \item detect cars
  \item detect stop signs
  \item detect traffic lights
\end{itemize}
Then the loss is $\frac{1}{m}\sum_{i=1}^m\sum_{j=1}^4\mathcal{L}(\hat{y}^{(i)}_j,y^{(i)}_j)$.
Note that in contrast to softmax regression here one image can have multiple labels.
If the data is incompletely labelled, one can just sum over the defined labels.
Multi-task learning makes sense when
\begin{itemize}
  \item training on a set of tasks that could benefit from having shared lower-level features
  \item usually: amount of data you have for each task is quite similar
  \item can train a big enough neural network to do well on all the tasks
\end{itemize}

\subsection{End-to-end deep learning}
\subsubsection{What is end-to-end deep learning}
End-to-end deep learning replaces a pipeline of multiple stages with a single neural network.
Deep learning requires a large dataset in order to have superior performance.
Otherwise one can only omit no or some stages.
In practise often you don't have enough data for the end-to-end approach.
Often you have more data for each of the sub-tasks.
\emph{E.g.} when detecting faces
\begin{itemize}
  \item detect location of face
  \item compared zoomed face to picture in database
\end{itemize}
Another example where end-to-end deep learning works is machine translation.

\subsubsection{Whether to use end-to-end deep learning}
Pros
\begin{itemize}
  \item let the data speak
  \item less hand-designing of components needed
\end{itemize}
Cons
\begin{itemize}
  \item may need large amount of data
  \item excludes potentially useful hand-designed components
\end{itemize}
Applying end-to-end deep learning
\begin{itemize}
  \item key question: do you have sufficient data to learn a function of the complexity needed to map x to y?
\end{itemize}

Ruslan Salakhutdinov: Unsupervised learning/pretraining using Restricted Boltzmann Machines,
now (with increased computing power available) standard backpropagation works better,
current research is unsupervised learning (variational autoencoders, Generative Adversarial Networks),
try different things and don't be afraid to tackle hard problems, also interesting is deep reinforcement learning.

\section{Convolutional Neural Networks}
\subsection{Foundations of Convolutional Neural Networks}
\subsubsection{Computer Vision}
Computer Vision Problems:
\begin{itemize}
  \item image classification (\emph{e.g.} cat or not cat)
  \item object detection (pose detection, multiple objects)
  \item neural style transfer
\end{itemize}
Deep learning on large images would require a large number of weights if a fully connected network was used.

\subsubsection{Edge Detection Example}
Edges are detected by convolving with a linear shift-invariant filter, \emph{e.g.} the vertical Prewitt filter
(1 1 1 0 0 0 -1 -1 -1), a vertical Sobel filter (1 2 1 0 0 0 -1 -2 -1), or a Scharr filter
(3 10 3 0 0 0 -3 -10 -3).
Convolution is denoted by $*$.
In Tensorflow convolution can be performed using the function ``tf.nn.conv2d''.
Instead of hand-picking the values of the filter, one can use machine learning to choose the individual weights of each filter.

\subsubsection{Padding}
You can use zero-padding so that the output image has the same size as the input.
Otherwise edge pixels are weighted lower and also the output of a deep neural networks becomes very small.
\begin{itemize}
  \item ``valid'': no padding, output image will be smaller; $p=0$
  \item ``same'': padding so that the output size is the same as the input size; $p=\frac{f-1}{2}$ where $f$ is the filter size
\end{itemize}
The filter size $f$ usually is an odd number.

\subsubsection{Strided Convolutions}
The result of the convolution can be subsampled by applying a stride larger than $1$.
The size of the output is $\lfloor\frac{n+2p-f}{s}+1\rfloor$.
Note that in machine learning convolution is actually a cross-correlation.
Convolution is sometimes preferred in signal processing because it is an associative operation $(A\ast B)\ast C=A\ast (B\ast C)$.

\subsubsection{Convolutions over Volumes}
An RGB image with $6\times 6\times 3$ elements gets convolved with a $3\times 3\times 3$ filter.
The output is a $4\times 4$ image.
When using multiple filters, the output will be three-dimensional (will have multiple channels).

\subsubsection{One Layer of a Convolutional Network}
A convolutional layer adds a bias unit for each filter followed by the ReLU activation function.
\emph{I.e.} a $3\times 3\times 3$ filter has $3^3+1=28$ parameters.
If layer $l$ is a convolutional layer:
\begin{itemize}
  \item $f^{[l]}$ is the filter size
  \item $p^{[l]}$ is the padding
  \item $s^{[l]}$ is the stride
  \item $n^{[l]}_C$ is the number of filters
\end{itemize}
The input is of size $n^{[l-1]}_H\times n^{[l-1]}_W\times n^{[l-1]}_C$.
The output and the activations are of size $n^{[l]}_H\times n^{[l]}_W\times n^{[l]}_C$.
Each of the $n^{[l]}$ filters is of size $f^{[l]}\times f^{[l]}\times n^{[l-1]}_C$.
\begin{equation}
  \begin{split}
    n^{[l]}_H&=\lfloor\frac{n^{[l-1]}_H+2p^{[l]}-f^{[l]}}{s^{[l]}}+1\rfloor\\
    n^{[l]}_W&=\lfloor\frac{n^{[l-1]}_W+2p^{[l]}-f^{[l]}}{s^{[l]}}+1\rfloor
  \end{split}
\end{equation}
When using batch gradient descent the activations $A^{[l]}$ will have size
$m\times n^{[l]}_H\times n^{[l]}_W\times n^{[l]}_C$.
The weights will have size $f^{[l]}\times f^{[l]}\times n^{[l-1]}_C\times n^{[l]}_C$.
The biases will have size $n^{[l]}_C$.

\subsubsection{Simple Convolutional Network Example}
As you go deeper into a network the height and width decreases while the number of channels increases.
Finally a fully connected logistic or softmax layer uses the stacked output of the last convolutional layer.
A convolutional neural network usually has the following layers
\begin{itemize}
  \item convolution (CONV)
  \item pooling (POOL)
  \item fully connected (FC)
\end{itemize}

\subsubsection{Pooling Layers}
Max-pooling takes the biggest number in each $2\times 2$ region ($f=s=2$).
Max-pooling does not have any parameters to learn.
Usually the channels are not reduced when doing max-pooling.
One can also use average pooling (less common).
The hyperparameters for pooling are $f$ (filter size) and $s$ (stride).
Max pooling usually does not use any padding.
If the input is of size $n_H\times n_W\times n_C$, the output is of size
$\lfloor\frac{n_H-f}{s}+1\rfloor\times\lfloor\frac{n_W-f}{s}+1\rfloor\times n_C$.

\subsubsection{CNN Example}
Often convolutional and max-pooling are seen as one layer (because the pooling layer has no weights).
Also see \cref{tbl:cnnexample}.
\begin{table}
  \begin{tabular}{c|crr}\toprule
    & activation shape & activation size & \#parameters\\\midrule
    input:           & (32, 32,  3) & 3072 &     0 \\
    CONV1 (f=5, s=1) & (28, 28,  8) & 6272 &   608 \\
    POOL1            & (14, 14,  8) & 1568 &     0 \\
    CONV2 (f=5, s=1) & (10, 10, 16) & 1600 &  3216 \\
    POOL2            & (5, 5, 16)   &  400 &     0 \\
    FC3              & (120, 1)     &  120 & 48120 \\
    FC4              & (84, 1)      &   84 & 10164 \\
    Softmax          & (10, 1)      &   10 &   850 \\\bottomrule
  \end{tabular}
  \caption{Example CNN\label{tbl:cnnexample}}
\end{table}

\subsubsection{Why Convolutions?}
The two advantages are
\begin{itemize}
  \item parameter sharing: a feature detector that's usefil in one part of the image is probably useful in another part of the image
  \item sparsity of connections: in each layer, each output value depends only on a small number of inputs
\end{itemize}

\subsection{Deep convolutional models: case studies}
\subsubsection{Why look at case studies?}
A good way to gain intuition about convolutional neural networks is to read/see other examples of effective conv nets.
Some classic networks are:
\begin{itemize}
  \item LeNet-5
  \item AlexNet
  \item VGG
\end{itemize}
More recent networks are
\begin{itemize}
  \item ResNet
  \item Inception
\end{itemize}

\subsubsection{Classic Networks}
\emph{LeCun et al., 1998, Gradient-based learning applied to document recognition}:
LeNet-5 classifies handwritten digits (MNIST) of size $32\times 32\times 1$ (gray scale).
LeNet-5 uses $5\times 5$ filters, average pooling, sigmoid and tanh activation functions,
and conv layers with increasing number of channels.
The final two layers are fully connected layers.
Modern networks rather use max-pooling and ReLU activation functions.
LeNet-5 has about 60,000 parameters.

\emph{Krizhevsky et al., 2012, ImageNet classification with deep convolutional neural networks}:
AlexNet is named after Alex Krizhevsky.
The input size is $227\times 227\times 3$.
It uses $11\times 11$ filters with a stride of $4$, later a $5\times 5$ filter with padding and stride $1$.
Max-pooling uses $3\times 3$ filters and a stride of $2$ in each case.
AlexNet uses the ReLU activation function.
There are five convolutional and three fully connected layers.
AlexNet as about 160,000,000 parameters and was implemented using multiple GPUs.
Local Response Normalization is used (but has not become popular).

\emph{Simonyan \& Zisserman 2015: Very deep convolutional networks for large-scale image recognition}:
The VGG-16 network just uses $3\times 3$ convolution filters using stride $1$ and same padding.
Max-pooling always uses $2\times 2$ filters with a stride of $2$.
Multiple convolutional layers are used before max-pooling.
The number of channels increases by a factor of $2$ with each group of convolutional layers.
Finally there are two fully connected layers.
Altogether there are $16$ layers.
The network has about 138,000,000 parameters.

\subsubsection{ResNets}
When using deeper networks the training error eventually goes up again.
Also deep networks are difficult to train because of vanishing/exploding gradient types of problems.
Residual networks (ResNets) use skip connections to take activations to another layer much deeper in the neural network.
A residual block uses a skip connection as follows
\begin{equation}
  \begin{split}
    z^{[l+1]}&=W^{[l+1]}a^{[l]}+b^{[l+1]}\\
    a^{[l+1]}&=g(z^{[l+1]})\\
    z^{[l+2]}&=W^{[l+2]}a^{[l+1]}+b^{[l+2]}\\
    a^{[l+2]}&=g(z^{[l+2]}+a^{[l]})
  \end{split}
\end{equation}
Also see \emph{He et al., 2015, Deep residual networks for image recognition}.
A neural network with $L$ layers can be converted to a network with $L/2$ residual blocks.

\subsubsection{Why ResNets Work}
If the weight $W^{[l+1]}$ and the bias $b^{[l+1]}$ in a residual block are zero and the ReLU activation function is used,
the residual block outputs $a^{[l]}$, \emph{i.e.} becomes the identity function.
It is easy for a residual block to learn the identity function.
Note that $z^{[l+2]}$ and $a^{[l]}$ need to have the same dimension.
If not, an adapter matrix $W_s$ is used so that $W_s a^{[l]}$ has the same dimension as $z^{[l+2]}$.
$W_s$ can be a diagonal matrix performing zero-padding or it can be learned.

\subsubsection{Network in Network and 1x1 Convolutions}
\emph{Lin et al., 2013, Network in network}:
A 1x1 convolution with a different value for each channel and a different filter for each output layer
is like defining a fully connected network for every pixel (each having multiple channels).
A network in network can be used to reduce the number of channels.
The filter size is $1\times 1\times n^{[l-1]}_C\times n^{[l]}_C$.

\subsubsection{Inception Network Motivation}
\emph{Szegedy et al., 2014, Going deeper with convolutions}:
One can perform different convolutions with different filter sizes as well as max-pooling and stack the output channels
(the width and height of the outputs must be the same).
To reduce the computational cost, one can use a ``bottleneck layer'' ($1\times 1$ convolution) to first shrink the number of channels.

\subsubsection{Inception Network}
\Cref{fig:inception} shows a module of the Inception network.
\begin{figure}[htbp]
  \begin{center}
    \includegraphics[width=\textwidth]{inception}
    \caption{Inception module}
    \label{fig:inception}
  \end{center}
\end{figure}
The Inception network uses many Inception blocks.
Also there are outputs (side-branches) in the middle of the network which are used to predict the output.
This appears to have a regularizing effect and helps to prevent this network from overfitting.

\subsection{Practical advices for using ConvNets}
\subsubsection{Using Open-Source Implementations}
Even after reading the paper it can be difficult to reproduce results published by another researcher.
Often researchers publish their implementation on Github.

\subsubsection{Transfer Learning}
Rather than training from scratch using random initialization
one can use transfer learning to transfer knowledge from one of the large public datasets.
One initializes the network using the weights from the public network.
Then one can freeze no part, part, or all but the last layer (depending on how much data you have) of the network and train the rest.
To speed up training even more one can precompute the responses of the frozen network for the training set.

\subsubsection{Data Augmentation}
Different data augmentation methods are
\begin{itemize}
  \item mirroring an image
  \item random cropping an image
  \item rotation, shearing, local warping
  \item random color shifting (\emph{e.g.} add RGB(20, -20, 20) to the image), PCA color augmentation
\end{itemize}
In practise often data augmentation and training run in parallel.
The data augmentation parameters are hyperparameters to optimize.

\subsubsection{State of Computer Vision}
There is (relative to the task) little data available for object detection.
There is (relative to the task) lots of data available for speech recognition.
When lots of data is available, one can use simpler algorithms with less hand-engineering.
\emph{I.e.} there are two sources of knowledge
\begin{itemize}
  \item labeled data
  \item hand engineered features/network architecture/other components/transfer learning
\end{itemize}
Tips for doing well on benchmarks/winning competitions
\begin{itemize}
  \item ensembling (train several networks independently and average their outputs)
  \item multi-crop at test time (run classifier on multiple versions of test images and average results)
\end{itemize}

\subsection{Detection algorithms}
\subsubsection{Object Localization}\label{cha:localize}
There are three different algorithms:
\begin{itemize}
  \item image classification: just classify the object
  \item classification with localization: classify and localize object in a picture
  \item detection: classify and localize multiple objects in a picture
\end{itemize}
When doing classification with localization one can use additional outputs $b_x, b_y, b_h, b_w\in[0,1]$ which define a bounding box
with midpoint $(b_x,b_y)$ and bounding box width $b_w$ and height $b_h$.
The output is
\begin{equation}
  y=\begin{pmatrix}p_c\\b_x\\b_y\\b_h\\b_w\\c_1\\c_2\\c_3\end{pmatrix}
\end{equation}
Where $p_c$ decides whether there is an object at all and $c_1,c_2,c_3$ are $1$ if the object belongs to the given class (\emph{e.g.} pedestrian, car, or motorcycle).
Mote that here we assume that there is at most one object shown in the picture.
When $p_c$ is $0$, the other parts of the label are ``?'' (don't care).
One can use a log-likelihood loss for the class labels, square error for the bounding box parameters,
and logistic regression loss for $p_c$.

\subsubsection{Landmark Detection}
One can learn landmark coordinates (\emph{e.g.} corners of left and right eye, or multiple keypoints on a face, pose of a person).
The network then would output $l_{1x},l_{1y},l_{2x},l_{2y},\ldots$.
The coordinates can for example be used for augmented reality.

\subsubsection{Object Detection}
First a neural network is trained to classify tightly cropped images to whether a car is shown in the image or not.
The conv net then is used with a sliding window to exhaustively search the image.
The process is repeated for different window sizes.
This algorithm is called ``sliding windows detection''.

\subsubsection{Convolutional Implementation of Sliding Windows}\label{cha:cnnslide}
\emph{Sermanet et al., 2014, OverFeat: Integrated recognition, localization and detection using convolutional networks}:
One can turn fully connected layers into $n\times n$ convolutional layers where $n$ is the width and height of the previous' layer activation.
The layer's activation has a width and height of $1$ (see \cref{fig:convslide} for an example with 4 classes).
Running the same algorithm on the full image instead of the slidding window, one obtains the results for all window positions
while reusing a lot of the computation which is identical between different overlapping slidding windows.
\begin{figure}[htbp]
  \begin{center}
    \includegraphics[width=\textwidth]{convslide}
    \caption{Implementing sliding window using $1\times 1$ convolutional layers}
    \label{fig:convslide}
  \end{center}
\end{figure}

\subsubsection{Bounding Box Predictions}\label{cha:bbpred}
The You Only Look Once (YOLO) algorithm divides up the image into a grid (\emph{e.g.} $19\times 19$).
The training label for an image for example is a $19\times 19\times 8$ (where $8$ is as in \cref{cha:localize}).
\emph{I.e.} for each grid cell there are $8$ output values.
The bounding box coordinates are specified relative to the grid cell.
Note that $b_w$ and $b_h$ can be bigger than one.

\subsubsection{Intersection Over Union}
Intersection over Union (IoU) is used to evaluate an object detection algorithm.
IoU is a measure of the overlap between two bounding boxes.
The function simply divides the area of the bounding box intersection by the area of the union.
The answer is ``correct'' if IoU$\ge$0.5.

\subsubsection{Non-max Suppression}\label{cha:nonmax}
Your algorithm may find multiple detections of the same object.
YOLO can detect the same object in multiple neighbouring cells.
The cell with the largest $p_c$ gets selected. Other bounding boxes with a high IoU are rejected.
The algorithm is as follows:
\begin{itemize}
  \item Discard all boxes with $p_c\le 0.6$.
  \item While there are any remaining boxes:
    \begin{itemize}
      \item Pick the box with the largest $p_c$.
      \item Discard any remaining box with IoU$\ge$0.5 with the box output in the previous step.
    \end{itemize}
\end{itemize}
Non-maxima suppression is performed for each output class.

\subsubsection{Anchor Boxes}
If there are overlapping objects, two object centres can end up in the same grid cell.
Each grid cell outputs two object classifications and bounding boxes.
Each object in training image is assigned to grid cell that contains object's midpoint and anchor box for the grid cell with highest IoU.
\emph{E.g.} one anchor box is wide and the other anchor box is tall.
The output of the algorithm is $19\times 19\times 16$.
The anchor boxes can be chosen using k-means clustering.

\subsubsection{YOLO Algorithm}
\emph{Redmon et al., 2016, You Only Look Once: Unified, Real-Time Object Detection}:
The YOLO algorithm works as follows.
The training labels are of size $19\times 19\times 2\times 8$ if using $2$ anchor boxes.
The $8$ elements are $p_c$, the $4$ bounding box coordinates, and the $3$ class label (pedestrian, car, motorcycle).
Then non-max suppressed outputs are produced
\begin{itemize}
  \item For each grid call, get $3$ predicted bounding boxes.
  \item Get rid of low probability predictions.
  \item For each class use non-max suppression to generate final predictions as shown in \cref{cha:nonmax}.
\end{itemize}

\subsubsection{Region Proposals}
\emph{Girshik et al., 2013, Rich feature hierarchies for accurate object detection and semantic segmentation}:
The R-CNN algorithm segments the scene into a small number of interesting blobs.
The convolutional network then is run on each region of interest (using a bounding box encapsulating the blob).
For each of the regions a label and accurate bounding box are produced.

\emph{Fast R-CNN} uses the convolution implementation of sliding windows (see \cref{cha:cnnslide}) to classify all the proposed regions.
\emph{Faster R-CNN} uses a convolutional neural network to propose regions.
However R-CNN tends to be slower than YOLO.

\subsection{Face Recognition}
\subsubsection{What is face recognition}
Example face recognition controlling the entrance gate at Baidu.
Face verification and face recognition are two different problems.
\begin{itemize}
  \item face verification: given image and name/ID output whether the input image is that of the claimed person
  \item face recognition: given a database of $K$ persons and an input image, output ID if the image is any of the $K$ persons (or ``not recognized'')
\end{itemize}
Face recognition is much harder because the required accuracy for each of the $K$ entries is much higher.
Face verification basically is face recognition with $K=1$.

\subsubsection{One Shot Learning}
In one-shot learning one learns a distance function $d$
which given a pair of images outputs a low value if they are from the same person and a high value otherwise.
Face recognition is performed by comparing the input image with all $K$ images in the database.
The function $d$ is trained using only one shot of each person.
Note that it is not required to retrain the algorithm if a new person's face is added to the database.

\subsubsection{Siamese Network}\label{cha:siamese}
\emph{Taigman et al., 2014, DeepFace closing the gap to human level performance}:
A siamese network $f$ is a network given to images $x^{(i)}$ and $x^{(j)}$ allows you to define the distance function as follows
\begin{equation}
  d(x^{(i)},x^{(j)}) = ||f(x^{(i)})-f(x^{(j)})||
\end{equation}
The output of $f$ is a feature vector (\emph{e.g.} 128 values).
The function $f$ is a convolutional neural network encoding the input image.
The parameters of the neural network define an encoding $f(x^{(i)})$.

\subsubsection{Triplet Loss}
\emph{Schroff et al., 2015, FaceNet: A unified embedding for face recognition and clustering}:
The Siamese network is trained using triplet loss.
A triplet consist of the
\begin{itemize}
  \item an anchor image A
  \item a positive image P
  \item a negative example N
\end{itemize}
The following property is desirable
\begin{equation}\label{equ:triplet}
  \underbrace{||f(A)-f(P)||^2}_{d^2(A,P)} + \alpha \le \underbrace{||f(A)-f(N)||^2}_{d^2(A,N)}
\end{equation}
where $\alpha>0$ is the margin.
The loss function $\mathcal{L}$ is defined as
\begin{equation}
  \mathcal{L}(A,P,N)=\max(||f(A)-f(P)||^2-||f(A)-f(N)||^2+\alpha, 0)
\end{equation}
The overall cost function is defined as
\begin{equation}
  J=\sum_{i=1}^m\mathcal{L}(A^{(i)},P^{(i)},N^{(i)})
\end{equation}
For the training set it is necessary to have multiple images of each person (otherwise $A$ and $P$ would be the same picture).
If A, P, and N are chosen randomly, the condition \cref{equ:triplet} is easily satisfied.
It is important to choose triplets that are ``hard'' to train on:
\begin{equation}
  d(A, P)\approx d(A, N)
\end{equation}
See Schroff et al. paper about FaceNet for details.

\subsubsection{Face Verification and Binary Classification}
Instead of using the distance $d$ one can learn a similarity function.
In this case face recognition is posed as a binary classification problem.
\emph{I.e.} the outputs of $f$ are compared using a final logistic regression unit.
\begin{equation}
  \hat{y}=\sigma(\sum_kw_k|f(x^{(i)})_k-f(x^{(j)})_k|+b)
\end{equation}
See DeepFace paper for more details.
Given a new image $x^{(i)}$ one can use precomputed $f(x^{(j)})$ for faster classification.

\subsection{Neural Style Transfer}
\subsubsection{What is neural style transfer?}
From a content image $C$ and a style image $S$ one generates an image $G$ which combines the content and style.

\subsubsection{What are deep ConvNets learning?}
\emph{Zeiler and Fergus, 2013, Visualizing and understanding convolutional networks}:
One can visualize what image patch maximizes a hidden unit's activation.

\subsubsection{Cost Function}
\emph{Gatys et al., 2015, A neural algorithm of of artistic style}:
\begin{equation}
  J(G)=\alpha J_{content}(C, G)+\beta J_{style}(S, G)
\end{equation}
The generated image $G$ is acquired as follows
\begin{enumerate}
  \item Initiate $G$ randomly
  \item Use gradient descent to minimize $J(G)$: $G\coloneqq G-\frac{\partial}{\partial G}J(G)$
\end{enumerate}

\subsubsection{Content Cost Function}
\begin{itemize}
  \item A hidden layer $l$ somewhere in the middle of the network is used for content cost
  \item Use pre-trained convnet (\emph{e.g.} VGG network)
  \item Let $a^{[l](C)}$ and $a^{[l](G)}$ be the activations of layer $l$ on the images.
  \item If $a^{[l](C)}$ and $a^{[l](G)}$ are similar, both images have similar content
\end{itemize}
\begin{equation}
  J_{content}(C, G) = \frac{1}{2} ||a^{[l](C)} - a^{[l](G)}||^2
\end{equation}

\subsubsection{Style Cost Function}
Say you are using layer $l$ to measure ``style''.
The style is defined as the correlation between activations across channels.
The layer is of size $n_H\times n_W\times n_C$.
Let $a^{[l]}_{i,j,k}$ be the activation at $(i,j,k)$.
The style matrix $G^{[l]}$ is of size $n_c^{[l]}\times n_c^{[]}$.
Note that the style matrix $G^{[l]}$ is not the same as the generated image $G$ (name clash).
\begin{equation}
  \begin{split}
    G^{[l](S)}_{kk^\prime}&=\sum_i\sum_j a^{[l](S)}_{ijk} a^{[l](S)}_{ijk^\prime}\\
    G^{[l](G)}_{kk^\prime}&=\sum_i\sum_j a^{[l](G)}_{ijk} a^{[l](G)}_{ijk^\prime}
  \end{split}
\end{equation}
The style cost function is
\begin{equation}
  J^{[l]}_{style}(S, G)=\frac{1}{(2n^{[l]}_Hn^{[l]}_Wn^{[l]}_C)^2}||G^{[l](S)}-G^{[l](G)}||^2_F
\end{equation}
A weighted sum of style cost functions of different layers is used to compute the full cost function
\begin{equation}
  J_{style}(S,G)=\sum_l \lambda^{[l]} J^{[l]}_{style}(S,G)
\end{equation}

\subsubsection{1D and 3D Generalizations}
ECG are examples of 1D data.
CAT scans are examples of 3D data.
Conv nets can be applied to 1D and 3D data analogous to 2D data.

\section{Sequence Models}
\subsection{Recurrent Neural Networks}
\subsubsection{Why sequence models}
Application of sequence models
\begin{itemize}
  \item speech recognition
  \item music generation
  \item sentiment classification
  \item DNA sequence analysis
  \item machine translation
  \item video activity recognition
  \item name entity recognition
\end{itemize}
In some cases input and output are sequences

\subsubsection{Notation}
The input sequences is $x^{<1>}, x^{<2>}, \ldots$.
The output sequences is $y^{<1>}, y^{<2>}, \ldots$.
The length of the input and output sequence are $T_x$ and $T_y$.
If there are multiple training examples, then we use $x^{(i)<t>}$, $y^{(i)<t>}$, $T_x^{(i)}$, and $T_y^{(i)}$
for the $i$th training example.
Words in text can be represented as one-hot vectors with for example 10,000 units.

\subsubsection{Recurrent neural network}
A standard neural network cannot be used for sequence data because
\begin{itemize}
  \item Inputs, outputs can be different lengths in different examples.
  \item Doesn't share features learned across different positions of text.
\end{itemize}
A recurrent neural networks uses information from the previous timestep.
At the first timestep a zero activation is used as information from the previous timestep.
\begin{equation}
  \begin{split}
    a^{<0>}&=\vec{0}\\
    a^{<t>}&=g_a(W_{aa}a^{<t-1>}+W_{ax}x^{<t>}+b_a)\\
    \hat{y}^{<t>}&=g_y(W_{ya}a^{<t>}+b_y)
  \end{split}
\end{equation}
For $g_a$ $\tanh$ and ReLU are frequently used.
$g_y$ can for example be the sigmoid function.
One can stack $W_{aa}$ and $W_{ax}$ horizontally and replace $W_{ya}$ with $W_y$ which results in:
\begin{equation}
  \begin{split}
    a^{<t>}&=g(W_a[a^{<t-1>},x^{<t>}]+b_a)\\
    \hat{y}^{<t>}&=g(W_ya^{<t>}+b_y)
  \end{split}
\end{equation}
where $[a^{<t-1>},x^{<t>}]$ is stacked vertically.

\subsubsection{Backpropagation through time}
The parameters $W_a, b_a, W_y, b_y$ are used in each time step.
One can use the standard logistic regression loss (cross-entropy loss) for each time step $t$:
\begin{equation}
  \mathcal{L}^{<t>}(\hat{y}^{<t>},y^{<t>})=-y^{<t>}\log\hat{y}^{<t>}-(1-y^{<t>})\log(1-\hat{y}^{<t>})
\end{equation}
The total loss is computed by summation over the time steps:
\begin{equation}
  \mathcal{L}(\hat{y},y)=\sum_{t=1}^{T_y}\mathcal{L}^{<t>}(\hat{y}^{<t>},y^{<t>})
\end{equation}
The most significant backpropagation is through time for the sequence of activations.

\subsubsection{Different types of RNNs}
\emph{Andrej Karpathy: The Unreasonable Effectiveness of Recurrent Neural Networks}:
In some cases $T_x\neq T_y$.
Examples of RNN architectures:
\begin{itemize}
  \item one-to-one (standard neural network)
  \item many-to-many (online/real-time) (\emph{e.g.} name entity recognition)
  \item many-to-one (\emph{e.g.} sentiment analysis)
  \item one-to-many (\emph{e.g.} music generation, image captioning)
  \item many-to-many (encoder and decoder part) with different input and output sizes (\emph{e.g.} machine translation)
\end{itemize}

\subsubsection{Language model and sequence generation}\label{cha:langmod}
A language model assigns a probability to each sentence.
The language model is trained using a large corpus of english text.
A sentence is represented using words and a final token: $y^{<1>}, y^{<2>},\ldots,\mathrm{<eos>}$.
Unknown words not in the vocabulary are represented using the token $\mathrm{<unk>}$.
The softmax output of the language RNN outputs the probabilities $\hat{y}^{<1>}$ of the first word.
Given the actually chosen first word $y^{<1>}$ and the activations of the previous step,
it then outputs the probabilities for the second word $\hat{y}^{<2>}$.
\emph{I.e.} the RNN learns to predict one word at a time.
The softmax loss function is
\begin{equation}
  \mathcal{L}(\hat{y}^{<t>},y^{<t>})=-\sum_i y^{<t>}_i \log \hat{y}^{<t>}_i
\end{equation}
The overall loss is
\begin{equation}
  \mathcal{L}=\sum_t\mathcal{L}^{<t>}(\hat{y}^{<t>},y^{<t>})
\end{equation}
After training on a large corpus of text,
one can \emph{e.g.} predict the probability of
\begin{equation}
  P(y^{<1>},y^{<2>},y^{<3>})=P(y^{<1>})P(y^{<2>}|y^{<1>})P(y^{<3>}|y^{<1>},y^{<2>})
\end{equation}

\subsubsection{Sampling novel sequences}
The RNN type of the language model is a many-to-many network using the previous word as an input.
The language model allows you to sample according to the softmax outputs of the RNN.
In Python one can use np.random.choice.
The end of the sentence is represented using the $\mathrm{<eos>}$ token.
One can reject the $\mathrm{<unk>}$ word.
It is also possible to build a character-level RNN instead of a word-level RNN.
The disadvantage is that character-level RNNs need to process longer sequences which makes them harder and more computationally expensive to train.

\subsubsection{Vanishing gradients with RNNs}
Similar to deep neural networks, RNNs suffer from the vanishing/exploding gradients problem.
Language has long term dependencies (\emph{e.g.} ``The cat, which ..., was full.'' vs. ``The cats, which ..., were full.'').
One can handle numerical problems caused by exploding gradients using gradient clipping.
The following sections show solutions for the vanishing gradients problem.

\subsubsection{Gated Recurrent Unit (GRU)}
\emph{Chung et al., 2014, Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling}:
GRUs (simplified) use a memory cell $c^{<t>}$ (here $c^{<t>}=a^{<t>}$).
$\tilde{c}^{<t>}$ are the new candidates for overwriting $c^{<t>}$.
$\Gamma_u$ is the update gate.
\begin{equation}
  \begin{split}
    \tilde{c}^{<t>}&=\tanh(W_c[c^{<t-1>},x^{<t>}]+b_c)\\
    \Gamma_u&=\sigma(W_u[c^{<t-1>},x^{<t>}]+b_u)\\
    c^{<t>}&=\Gamma_u*\tilde{c}^{<t>}+(1-\Gamma_u)*c^{<t-1>}\\
    a^{<t>}&=c^{<t>}
  \end{split}
\end{equation}
where ``*'' is the elementwise multiplication.
The full GRU uses an additional gate $\Gamma_r$ for emphasizing relevant units.
\begin{equation}
  \begin{split}
    \tilde{c}^{<t>}&=\tanh(W_c[\Gamma_r*c^{<t-1>},x^{<t>}]+b_c)\\
    \Gamma_u&=\sigma(W_u[c^{<t-1>},x^{<t>}]+b_u)\\
    \Gamma_r&=\sigma(W_r[c^{<t-1>},x^{<t>}]+b_r)\\
    c^{<t>}&=\Gamma_u*\tilde{c}^{<t>}+(1-\Gamma_u)*c^{<t-1>}\\
    a^{<t>}&=c^{<t>}
  \end{split}
\end{equation}
GRUs are much better at capturing long term dependencies.

\subsubsection{Long Short Term Memory (LSTM)}
\emph{Hochreiter and Schmidhuber, 1997, Long short-term memory}:
LSTMs have an update gate $\Gamma_u$, a forget gate $\Gamma_f$, and an output gate $\Gamma_o$.
\begin{equation}
  \begin{split}
    \tilde{c}^{<t>}&=\tanh(W_c[a^{<t-1>},x^{<t>}]+b_c)\\
    \Gamma_u&=\sigma(W_u[a^{<t-1>},x^{<t>}]+b_u)\\
    \Gamma_f&=\sigma(W_f[a^{<t-1>},x^{<t>}]+b_f)\\
    \Gamma_o&=\sigma(W_o[a^{<t-1>},x^{<t>}]+b_o)\\
    c^{<t>}&=\Gamma_u*\tilde{c}^{<t>}+\Gamma_f*c^{<t-1>}\\
    a^{<t>}&=\Gamma_o*\tanh c^{<t>}
  \end{split}
\end{equation}
A variation of the LSTM uses ``peephole connections'', making the gates depend on $c^{<t-1>}$ as well.

\subsubsection{Bidirectional RNN}
A sentence cannot in general be understood by looking backwards in time only.
\emph{E.g.} the meaning of the third word Teddy can only be interpreted correctly after reading the next word.
\begin{itemize}
  \item He said, ``Teddy bears are on sale!''
  \item He said, ``Teddy Roosevelt was a great president!''
\end{itemize}
A bidirectional RNN (BRNN) works using forward activations and backward activations.
The activations are connected to the output using a fully connected layer.

\subsubsection{Deep RNNs}
One can stack multiple layers of RNNs to build deeper RNNs.
There are activations $a^{[l]<t>}$ for each layer $l$ and each timestep $t$.
\begin{equation}
  a^{[l]<t>}=g(W_a^{[l]}[a^{[l]<t-1>},a^{[l-1]<t>}]+b_a^{[l]})
\end{equation}

\subsection{NLP and Word Embeddings}
\subsubsection{Word Representations}
Words can be represented using a vocabulary and one-hot vectors.
However one-hot representations do not generalise well when learning language
(the distance between any two words is the same).
Word embeddings are a featurized representation of words.
One can use t-SNE to visualise clusters of words.

\subsubsection{Using word embeddings}
To solve the named entity recognition task one can use a word embedding trained on a large number of words.
The name entity recognition then can use the word vectors to train named entities on a small dataset (transfer learning)
using a BRNN.
Word embeddings create dense vectors and thus allows for a smaller model.
Optionally one can finetune the word embeddings (if the task has a big dataset).
Word embeddings are similar to face encodings (see \cref{cha:siamese}).

\subsubsection{Properties of word embeddings}
Word embeddings can be used to reason about analogies.
\emph{E.g.} man is to woman as king is to queen.
The word embeddings have similar differences: $e_{man}-e_{woman}\approx e_{king}-e_{queen}$.
Word similarity between $u$ and $v$ is defined as $\frac{u^\top v}{||u||_2||v||_2}$.

\subsubsection{Embedding matrix}
The embedding matrix has the embedding of a word in each column.
Multiplying the matrix with a word's one-hot vector returns the word embedding.
\begin{equation}
  E\cdot o_j=e_j=\mathrm{embedding\ for\ word\ j}
\end{equation}

\subsubsection{Learning word embeddings}
One can map words from a sentence using their embeddings and then
use a neural network with a softmax classifier to try to predict the next word in the text.
Optimizing the predictor yields the weights of a predictor as well as the word embedding matrix $E$.

\subsubsection{Word2Vec}
\emph{Mikolov et al., 2013, Efficient estimation of word representations in vector space}:
Skip-grams given a context word randomly pick a nearby target word to predict.
The network for learning embeddings works as follows
\begin{equation}
  \hat{y}=softmax(\underbrace{Eo_c}_{e_c})
\end{equation}
The input is a one-hot vector $o_c$.
The output of the softmax unit is
\begin{equation}
  p(t|c)=\frac{e^{\theta_t^\top e_c}}{\sum_je^{\theta_j^\top e_c}}
\end{equation}
The loss function is $\mathcal{L}(\hat{y},y)=-\sum_iy_i\log\hat{y}_i$.
$E$ and $\sigma$ are parameters to train.
In the literature a hierarchical softmax is used for performance reasons.
The sampling of the context is performed such that frequent words are not sampled much more often than rare words.

\subsubsection{Negative Sampling}
\emph{Mikolov et al., 2013, Distributed representation of words and phrases and their compositionality}:
Negative sampling is a modified learning problem which is much more efficient (no softmax).
Given a context word $c$, a positive target $t$ (label $1$) in the vincinity is identified
and $k$ random negative examples for $t$ are also selected (label $0$).
$k$ is small for large datasets and bigger for small datasets.
The learning problem then is to predict the label given the context and label as input.
The model is
\begin{equation}
  P(y=1|c,t)=\sigma(\theta_t^\top e_c)
\end{equation}
The parameters of the model are $E$ and $\theta$ (one vector $\theta_t$ for each word $t$).
The $k$ Words are sampled with the observed frequency of the word to the power of $3/4$.

\subsubsection{GloVe word vectors}
\emph{Pennington et al., 2014, GloVe: Global vectors for word representation}:
\begin{equation}
  X_{ij}=\mathrm{\#\ times\ i\ appears\ in context\ of\ j}
\end{equation}
GloVe minimizes the following model
\begin{equation}
  \mathrm{minimize\ }\sum_i\sum_jf(X_{ij})(\theta_i^\top e_j + b_i + b_j - \log X_{ij})^2
\end{equation}
$f$ is a weighting term. $f$ is zero if $X_{ij}$ is zero.
Note that $\theta_i$ and $e_j$ are symmetric.
One can choose the final parameter as follows: $e_w^{(final)}=\frac{e_w+\theta_w}{2}$.
The algorithm will in general choose axes which are not aligned with the coordinate axes of the vector space.

\subsection{Applications using Word Embeddings}
\subsubsection{Sentiment Classification}
The input $x$ is a piece of text, the output $y$ is the star rating.
The words get converted to vectors by multiplying with the embedding $E$.
A RNN with a softmax classifier then is used for sentiment analysis.

\subsubsection{Debiasing word embeddings}
\emph{Bolukbasi et. al, 2016. Man is to computer programmer as woman is to homemaker? Debiasing word embeddings}:
Learned word embeddings can learn a gender bias.
\emph{E.g.} ``Man is to computer programmer as woman is to homemaker.'' or
``Father is to doctor as mother is to nurse.''.
Word embeddings can reflect gender, ethnicity, age, sexual orientation, and other biases of the text used to train the model.
\begin{enumerate}
  \item Identify bias direction (\emph{e.g.} compute $e_{he}-e_{she}$, $e_{male}-e_{female}$, ... and take average
    (in the paper a SVD is used to identify principal components)
  \item Neutralize: For every word that is not definitional, project to get rid of bias.
  \item Equalize pairs (\emph{i.e.} gender words (\emph{e.g.} grandmother/grandfather, girl/boy, ...)
    should have the same distance from the projection plane.
\end{enumerate}

\subsection{Various sequence to sequence architectures}
\subsubsection{Basic Models}
\emph{Sutskever et al., 2014, Sequence to sequence learning with neural networks} and
\emph{Cho et al., 2014, Learning phrase representations using RNN encoder-decoder for statistical machine translation}:
One can build a encoder network which finds an encoding of a sentence.
A decoder network then generates the translation.

\emph{Mao et al., 2014, Deep captioning with multimodal recurrent neural networks}:
Image captioning works in a similar fashion.
One can use the pretrained AlexNet (without the final softmax layer) and feed it to an RNN to generate a caption.

\subsubsection{Picking the most likely sentence}
In machine translation, instead of sampling the distribution of sentences (as in \cref{cha:langmod}),
one wants to find the most likely translation for the given input.
\begin{equation}
  \mathop{\operatorname{argmax}}_{y^{<1>},\ldots,y^{<T_y>}}P(y^{<1>},\ldots,y^{<T_y>}|x)
\end{equation}
It is desired to pick the sentence that maximizes the joint probability of the whole sentence.

\subsubsection{Beam Search}
Beam search is the most widely used algorithm for selecting the most likely translation.
While greedy search would pick the most likely word when starting the sentence,
beam search will consider the $B$ out of $n$ most likely choices (\emph{e.g.} $B=3$) output by the softmax classifier.
Then the $n$ probabilities of the second word are evaluated for each choice of first word yielding $B*n$ choices.
Then again all but the most likely $B$ choices are discarded.
Then beam search continues to the next word.
In the special case of $B=1$, beam search is the same as a greedy search.
Production systems use something like up to $B=10$ whilst research publications use larger values of $B$.
$B$ is sometimes called the beam width.

\subsubsection{Refinements to Beam Search}
Instead of maximising the product of probabilities
\begin{equation}
  \mathop{\operatorname{argmax}}_y \prod_{t=1}^{T_y} P(y^{<t>}|x,y^{<1>},\ldots,y^{<t-1>})
\end{equation}
in practice on minimizes the sum of the log-probabilities to avoid numerical underflow
\begin{equation}
  \mathop{\operatorname{argmax}}_y \sum_{t=1}^{T_y} \log P(y^{<t>}|x,y^{<1>},\ldots,y^{<t-1>})
\end{equation}
Furthermore the problem is that beam search is biased towards short sentences because they are assigned higher probabilities.
To address this, one performs length normalization of the log-likelihood
\begin{equation}
  \mathop{\operatorname{argmax}}_y \frac{1}{T_y^\alpha}\sum_{t=1}^{T_y} \log P(y^{<t>}|x,y^{<1>},\ldots,y^{<t-1>})
\end{equation}
using \emph{e.g.} $\alpha=0.7$.

\subsubsection{Error analysis in beam search}
The beam search algorithm is a heuristic which does not always yield the best or correct result.
To identity errors caused by beam search as opposed to errors caused by the RNN,
one can compute $P(y^*|x)$ of the correct/desired result $y^*$ and $P(\hat{y}|x)$ of the result $\hat{y}$ returned by the RNN.
If $P(y^*|x)$ is higher than $P(\hat{y}|x)$, then the beam search is failing to return the correct result.
Note that the optimization objective should be used instead of $P$ when using length normalization.

The error analysis process is to examine a set of examples where the algorithm failed and decide using above criteria
to check whether beam search or the RNN failed.

\subsubsection{Bleu score}
\emph{Papineni et al., 2002, A method for automatic evaluation of machine translation}:
Language translation often has multiple right answers.
The bilingual evaluation understudy (Bleu) score.
Precision $p_1$ counts how many words (unigrams) of the machine translation are present in the reference
divided by the number of words in the machine translation.
If a word occurs multiple times in the machine translation, it only scores up to the maximum number of occurences in a reference sentence.
In the same way the Bleu score $p_2$ for bigrams (two consecutive words in the machine translation) is defined.
$p_n$ is the corresponding score for n-grams.
The combined Bleu score is $bp\cdot e^{\frac{1}{4}\sum_{n=1}^4 p_n}$
where $bp$ is the brevity penalty.
\begin{equation}
  bp=\left\{\begin{array}{l}1\ \mathrm{if}\ MT\_output\_length > reference\_output\_length\\
  exp(1-MT\_output\_length/reference\_output\_length)\ \mathrm{otherwise}\end{array}\right.
\end{equation}
The Bleu score is a \emph{single real-number evaluation metric} which helps comparing and developing machine translation algorithms.
The Bleu score can also be used for image captioning where multiple different captions can be valid.

\subsubsection{Attention Model Intuition}
\emph{Bahdanau et al. 2014, Neural machine translation by jointly learning to align and translate.}:
It is difficult for a decoder-encoder architecture to remember very long sentences.
\emph{I.e.} the Bleu score for long sentences goes down.
One can use a BRNN (bidirectional RNN) in the first layer and then compute attention values to define the context for a second RNN layer.

\subsubsection{Attention Model}
The attention model uses attention values $\alpha^{<t,t^\prime>}$ as shown in \cref{fig:attention}.
\begin{figure}[htbp]
  \begin{center}
    \includegraphics[width=\textwidth]{attention}
    \caption{A BRNN and RNN using attention values}
    \label{fig:attention}
  \end{center}
\end{figure}
Similar to softmax activations, the attention values are defined as
\begin{equation}
  \alpha^{<t,t^\prime>}=\frac{exp(e^{<t,t^\prime>})}{\sum_{t^\prime=1}^{T_x}exp(e^{<t,t^\prime>})}
\end{equation}
so that the values $\alpha^{<t,t^\prime>}$ add up to $1$ when summing over $t^\prime$.
$a^{<t^\prime>}$ and $s^{<t-1>}$ serve as input for a small neural network to compute $e^{<t,t^\prime>}$.

\subsection{Speech recognition}
\subsubsection{Speech recognition}
Given an audio clip $x$ one has to automatically find a transcript $y$.
A common preprocessing step for audio data is to make a spectrogram.
Using end-to-end learning it is not necessary to use phonemes (intermediate representation) any more.
However typically 3,000 hours of transcribed audio data (or even much more) is used.

\emph{Graves et al. 2006, Connectionist Temporal Classification: Labeling unsegmented sequence data with recurrent neural networks}:
Speech recognition algorithms can be trained using the Connectionist temporal classification (CTC) cost.
The audio input is usually split into small segments (\emph{e.g.} 10 milliseconds).
Repeated output characters not separated by ``blank'' are collapsed.
\emph{E.g.} ttt\_h\_eee\_\_\_\textvisiblespace\_\_\_qqq\_\_ becomes ``the\textvisiblespace q''.
The neural network then is allowed to repeat a character many times.

\subsubsection{Trigger Word Detection}
Trigger words are used to wake up a speech recognition device.
The RNN is a many-to-many network.
The output label for each audio frame is ``0'' by default and then ``1'' a few times after the trigger word was said.
(``1'' is specified multiple times so that the labels are more balanced).

\subsection{Conclusion}
The topics in this course were
\begin{itemize}
  \item Neural Networks and Deep Learning
  \item Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization
  \item Structuring Machine Learning Projects
  \item Convolutional Neural Networks
  \item Sequence Models
\end{itemize}
We hope that you will use deep learning to improve the world.

\appendix

\section{Reinforcement Learning}
\subsection{Markov Decision Process}
See lecture at \url{https://www.youtube.com/watch?v=RtxI449ZjSc} (Lecture 16-20, Machine learning, Andrew Ng).
Reinforcement learning uses a reward function for sequences of actions.
The problem in reinforcement learning is that generally there is a delay between an action and the reward.
In particular the \emph{credit assignment problem} is that the algorithm has to figure out
which action in a sequence of actions caused the reward/penalty.
\emph{E.g.} in an car accident stepping on the brakes does not cause a crash, it is rather something you have done before that.
The Markov Decision Process $MDP(S,A,P_{sa},\gamma,R)$ comprises of
\begin{itemize}
  \item $S$ is a set of states.
  \item $A$ is a set of actions.
  \item $P_{sa}$ are the state transition distributions $\sum_{s^\prime} P_{sa}(s^\prime)=1$, $P_{sa}(s^\prime)\ge 0$,
    given a state $s$ and an action $a$ the value $P_{sa}(s^\prime)$ is the probability of ending up in state $s^\prime$.
  \item $\gamma$ is the discount factor ($0 \le \gamma < 1$).
  \item $R$ is the reward function $R : S \mapsto \mathbb{R}$
\end{itemize}
It is common for navigation tasks to have +1 for the goal, -1 for crashes,
and a small negative reward for other states (\emph{i.e.} penalising battery use).
One can add a ``zero-cost absorbing state'' to model termination after reaching the goal.

\subsection{Bellman equation}
The robot starts at state $s_0$.
Given an action $a_0$ the robot goes into state $s_1 \sim P_{s_0 a_0}$.
Then $a_1$ is chosen and the robot transitions into state $s_2 \sim P_{s_1 a_1}$, etc.
The cumulative reward (total payoff) is $R(s_0)+\gamma R(s_1)+\gamma^2 R(s_2)+\ldots$.
The goal of the reinforcement algorithm is to choose actions over time ($a_0,a_1,\ldots$)
to maximise the expectation value of the total payoff $E[R(s_0)+\gamma R(s_1)+\gamma^2 R(s_2)+\ldots]$ and
to come up with a policy $\Pi:S\mapsto A$.
For any $\Pi$ define $V^\Pi:S\mapsto \mathbb{R}$.
$V^\Pi(s)$ is the expected payoff starting in state $s$ and executing $\Pi$.
\begin{equation}
  V^\Pi(s)=E[R(s_0)+\gamma R(s_1)+\ldots|\Pi,s_0=s]
\end{equation}
Note that
\begin{equation}
  V^\Pi(s)=E[R(s_0)+\gamma(\underbrace{R(s_1)+\gamma R(s_2)+\ldots}_{V^\Pi(s_1)})|\Pi,s_0=s]
\end{equation}
where $R(s_0)$ is the ``immediate reward'' and the other rewards are called ``future rewards''.
$V^\Pi(s)$ can be written recursively (using $s_0=s$ and $s_1=s^\prime$) when using the policy $\Pi$ as follows:
\begin{equation}
  V^\Pi(s)=R(s)+\gamma \sum_{s^\prime} P_{s\Pi(s)}(s^\prime) V^\Pi(s^\prime)
\end{equation}
This is called the ``Bellman equation''.
Given $\Pi$ the equation for each state can be used as an equation system to solve for all $V^\Pi(s)$.
The optimal value function is defined as the optimal value over all policies $\Pi$:
\begin{equation}
  V^\ast(s)=\max_\Pi V^\Pi(s)
\end{equation}
The policy $\Pi$ needs to choose the best action. \emph{I.e.}:
\begin{equation}
  V^\ast(s)=R(s)+\max_a \gamma \sum_{s^\prime} P_{sa}(s^\prime) V^\ast(s^\prime)
\end{equation}
The optimal policy is
\begin{equation}
  \Pi^\ast(s)=\mathop{\operatorname{argmax}}_a \sum_{s^\prime} P_{sa}(s^\prime)V^\ast(s^\prime)
\end{equation}

\subsubsection{Iterative algorithms}
The \emph{value iteration} algorithm works as follows
\begin{itemize}
  \item Initialize $V(s)=0\mathrm{\ }\forall s\in S$
  \item For every $s$ update $V(s)\coloneqq R(s) + \max_a \gamma \sum_{s^\prime} P_{sa}(s^\prime) V(s^\prime)$
  \item Repeat previous step
\end{itemize}
This will make $V(s)$ converge on $V^\ast(s)$.
You can update all $V(s)$ synchronously or asynchronously (sequentially).

The other standard algorithm is \emph{policy iteration}
\begin{itemize}
  \item Initialize $\Pi$ randomly
  \item Let $V\coloneqq V^\Pi$ (solve Bellman's equations)
  \item Let $\Pi(s)\coloneqq\mathop{\operatorname{argmax}}_a \sum_{s^\prime} P_{sa}(s^\prime)V(s^\prime)$
\end{itemize}
This will make $V$ converge on $V^\ast$ and $\Pi$ on $\Pi^\ast$.
For large MDPs one tends to use value iteration because it does not require solving a large equation system.

\subsection{What if transition probability not known?}
What if $P_{sa}(s^\prime)$ is not known?
In this case $P_{sa}$ is estimated from data.
\emph{I.e.} divide number of times action $a$ in $s$ and got to $s^\prime$ by
number of times took action $a$ in state $s$ (initially defaulting to uniform distribution if number of times is zero).
Putting it together:\\
Repeat
\begin{itemize}
  \item Take action using $\Pi$ to get experience in MDP.
  \item Update estimates of $P_{sa}$
  \item Solve Bellman's equations or use value iteration to get $V$
  \item update your policy $\Pi(s)=\mathop{\operatorname{argmax}}_a \sum_{s^\prime} P_{sa}(s^\prime) V(s^\prime)$
\end{itemize}

\subsection{Continuous states}
The state space $S$ can be continuous.
A car for example has position, rotation, speed, and angular speed (6 dof).
A helicopter has twelve degrees of freedom.
The inverted pendulum is a long-running classic in reinforcement learning has four degrees of freedom.
Discretizing the state space leads to inaccuracies and also suffers from the curse of dimensionality (lot of combinations).
The solution is to approximate $V^\ast$ more directly.
Here the action space $A$ stays discrete (the action space usually is discretized much more easily than the state space).
Further assumption is that you have a model/simulator of the MDP (Markov Decision Process).
The model takes $s$ and $a$ as input and outputs a new state $s^\prime$.
\emph{E.g.} a deterministic model $s_{t+1}=s_t+\Delta t\cdot\ldots$.
Alternatively one can learn a model to estimate the state vector $s_{t+1}$ as a function of $s_t$ and $a_t$
from multiple sequences of states and actions (for example captured while manually controlling the system).
For example using model
\begin{equation}
  s_{t+1}=As_t+Ba_t
\end{equation}
one minimizes
\begin{equation}
  \mathop{\operatorname{argmin}}_{A,B}\sum_{i=1}^m\sum_{t=0}^{T-1}||s^{(i)}_{t+1}-(As^{(i)}_t+Ba^{(i)}_t)||
\end{equation}
Using a normally distributed error $\epsilon_t\sim\mathcal{N}(0,\mathcal{E})$ one can implement a stochastic model
\begin{equation}
  s_{t+1}=As_t+Ba_t+\epsilon_t
\end{equation}
One chooses a vector of features $\phi(s)$ for $s$.
The value function gets approximated as $V(s)=\Theta^\top\phi(s)$.
Value iteration is
\begin{equation}
  V(s)\coloneqq R(s)+\gamma\max_a\sum_{s^\prime}P_{sa}(s^\prime)V(s)=
  R(s)+\gamma\max_a E_{s^\prime\sim P_sa}[V(s^\prime)]
\end{equation}
\subsection{Fitted value iteration}\label{cha:fittedvi}
Using the approximation $V(s)=\Theta^\top\phi(s)$ one can implement the following algorithm for continuous state space $S$.
\begin{itemize}
  \item Sample $\{s^{(1)},s^{(2)},\ldots,s^{(m)}\}\in S$ randomly.
  \item Initialise $\Theta\coloneqq 0$
  \item Repeat
    \begin{itemize}
      \item For $i=1,\ldots,m$
        \begin{itemize}
          \item For each action $a\in A$
            \begin{itemize}
              \item Sample $s^\prime_1,\ldots,s^\prime_k\sim P_{s^{(i)}a}$ (using model)
              \item Let $q(a)=\frac{1}{k}\sum_{j=1}^k [R(s^{(i)})+\gamma V(s^\prime_j)]$
                be estimate for $R(s^{(i)})+\gamma E_{s\prime\sim P_{s^{(i)}a}}[V(s^\prime)]$
            \end{itemize}
          \item Set $y^{(i)}=\max_a q(a)$ (estimate for $R(s^{(i)})+\gamma\max_a E_{s\prime\sim P_{s^{(i)}a}}[V(s^\prime)]$)
        \end{itemize}
      \item want $V(s^{(i)})\approx y^{(i)}$
      \item $\Theta\coloneqq\mathop{\operatorname{argmin}}_\Theta\frac{1}{2}\sum_{i=1}^m(\Theta^\top\phi(s^{(i)})-y^{(i)})^2$
    \end{itemize}
\end{itemize}
If the model/simulator is deterministic, one can use $k=1$.

\subsection{Policy with continuous state space}
The optimal policy given $V^\ast$ is the same as in the discrete case:
\begin{equation}
  \Pi^\ast(s)=\mathop{\operatorname{argmax}}_a E_{s^\prime\sim P_{sa}}[V^\ast(s^\prime)]
\end{equation}
In the continuous case it is not possible to compute $\Pi^\ast$ ahead of time any more.
Again the expectation is approximated by using $k$ samples (see \cref{cha:fittedvi}).
If $P_{sa}$ is deterministic, one can use $s^\prime=f(s,a)$ to simplify as follows
\begin{equation}
  \Pi^\ast(s)=\mathop{\operatorname{argmax}}_a V^\ast(f(s,a))
\end{equation}

Another common case is a stochastic simulator with zero mean Gaussian noise $\epsilon_t$
\begin{equation}
  s_{t+1}=f(s_t,a_t)+\epsilon_t
\end{equation}
\emph{E.g.} the linear system $s_{t+1}=As_t+Ba_t+\epsilon_t$.
In the stochastic simulator one can approximate the expected value for the value function as follows:
\begin{equation}
  E_{s^\prime\sim P_{sa}}[V^\ast(s^\prime)]\approx V^\ast(E[s^\prime])=V^\ast(f(s,a))
\end{equation}
The policy then again is
\begin{equation}
  \Pi^\ast(s)=\mathop{\operatorname{argmax}}_a\underbrace{V^\ast(f(s,a))}_{\Theta^\top\phi(f(s,a))}
\end{equation}

\end{document}
